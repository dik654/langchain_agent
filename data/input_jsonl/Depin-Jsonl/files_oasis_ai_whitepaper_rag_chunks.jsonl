{"text": "Oasis.AI Whitepaper Version 1.0 1 Introduction Over the past decade, machine learning has experienced a rapid growth in popularity, permeating various aspects of daily life. As the demand for compute resources for machine learning tasks continues to surge, existing centralized inference mechanisms are being pushed to their limits. Simultaneously, supply chain shortages have led to year-long back orders on cutting edge hardware while hundreds of thousands of users already have mid-tier to high-end excess compute sitting idle. By connecting this unused capacity via a distributed network, a higher global compute eﬃciency can be achieved. This approach not only ensures cost-eﬃciency but also delivers reliability and performance to a broader spectrum of users. The Oasis.ai machine learning inference platform oﬀers distributed Machine Learning as a Service (MLaaS) capabilities. The platform enables individuals to securely execute machine learning inference tasks with an expanding repertoire of popular models such as Llama [1] and Stable Diﬀusion XL [2], among others. 2 WebGPU-based Machine Learning Despite the clear beneﬁts of a distributed AI inference platform, the implementation of such a network requires a standardized system for all potential compute nodes to execute. Existing solutions have levered Docker as a means of deploying work units to service providers, oﬀering decentralized physical infrastructure networks (DePIN), which presents its own issues. Despite the ﬂexibility that this approach oﬀers, it comes at the cost of ease of setup and deployment as well opening up additional attack vectors via running unveriﬁed code. By focusing only on ML inference, rather than generic compute, a custom platform can be implemented in the browser by leveraging targeted APIs. Utilizing existing WebGPU APIs for ML represents a signiﬁcant advancement for large scale, distributed, edge compute by exposing low-level graphics and general purpose computation [3]. Via these APIs, web applications are able to harness the full power of modern GPUs for accelerated computing tasks, which includes machine learning inference. This advancement allows for the ﬂexibility to run larger, previously desktop-bound, machine learning model in an existing, proven, browser environment. Moreover, WebGPU oﬀers cross-platform support for hardware acceleration, allowing for consistent performance and implementation across a variety of devices. Thus, WebGPU is well-poised as a tool for browser-based MLaaS for making inference accessible for a wide range of applications. The Oasis.ai platform leverages WebGPU technology to enable eﬃcient and scalable distributed machine learning within a web browser environment. Providers are able to exchange access to their WebGPU compute for Oasis.ai token. Speciﬁcally, service providers run inference tasks with WebGPU on behalf of users in exchange for token. The emergence of browser-based machine learning signiﬁes a substantial leap in the access of AI technologies, oﬀering low-latency and ﬂexible processing directly within existing web applications. 1", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 1, "chunk_id": "files_oasis_ai_whitepaper_p1_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "3 Task Allocation Inference tasks can be allocated to service providers in exchange for tokens. Speciﬁcally, this model involves users exchanging tokenized tasks or requests in the form of digital assets, which can serve as a tradable unit of value for the execution of a speciﬁc machine learning model by service providers. Machine learning service providers can participate in the network by oﬀering their computational resources to run inference on an input, and they are rewarded with tokens in exchange. A signiﬁcant challenge in designing a task-allocation system is that determining the optimal task allocation solution is NP-Hard [4]. We implement the diﬀerential privacy-based combinatorial double auction algorithm proposed by Zhai et al. between users and service providers, and trades are executed when the bid price of a user matches the seller’s ask price [5]. 4 Combinatorial Double Auction Algorithm Consider a vector of service providers 𝑹= ⟨𝑅1, 𝑅2, ..., 𝑅𝑛⟩ and inference tasks 𝑻= ⟨𝑇1, 𝑇2, ..., 𝑇𝑚⟩. Additionally, each inference task should be represented as 𝑇𝑖= ⟨𝐶𝑖,1𝑡1, 𝐶𝑖,2𝑡2, ...𝐶𝑖,𝑘𝑡𝑘⟩ where 𝐶𝑖,𝑗 represents the demand for each of the 𝑘 inference task types (e.g. each of the machine learning model types). Let 𝑣𝑖 represent the adjusted valuation of computing the sub-task 𝑇𝑖, which reﬂects factors imposed by the demand such as the slashing mechanism discussed in Section 6. Let 𝑺 be the set 𝑆𝑗= (𝒔𝒋, 𝑤𝑗, 𝑐𝑗) where 𝒔𝒋 represents the 𝑗- th resource (e.g. CPU processing power). 𝑤𝑗 represents the maximum number of units of service that device 𝑗 can provide for each resource type 𝑠𝑗,𝑖 and 𝑐𝑗 represents the true adjusted valuation of the 𝑖-th device’s cost. We can construct a matrix 𝐴∈𝑀{𝑚×𝑛}(𝟙) such that 𝐴𝑖,𝑗= {1 𝑅𝑗 accepts the 𝑖-th task 0 else Indeed, ∑𝑚 𝑘=1 𝐴𝑘,𝑗≤1, for 1 ≤𝑗≤𝑛 since a model can strictly accept only one model inference. Let the additional cost function be deﬁned by 𝑒𝑖= 𝑡𝑖log𝛼(𝑡𝑖) + 𝛽 where 𝛼, 𝛽∈ℤ+. Then, a utility function for users can be deﬁned as price𝑖+ 𝑒𝑖) if the 𝑖-th task wins utility𝑢𝑖= {𝑣𝑖−(trade 0 else Similarly, the utility function for service providers can be deﬁned as 𝑖=1 𝐴𝑖,𝑗 if the 𝑗-th task wins utility𝑝𝑗= {trade price𝑗−∑𝑛 0 else We must optimize the function max(∑𝑚 𝑖=1 utility𝑢𝑖−∑𝑛𝑗=1 utility𝑝𝑗) such that: 𝑚 ∑ 𝐴𝑖,𝑗≤𝑤𝑗For each user device 𝑖=1 𝐴𝑖,𝑗∈[0, 1], ∀𝑖∈[1, 𝑚], 𝑗∈[1, 𝑛] 2", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 2, "chunk_id": "files_oasis_ai_whitepaper_p2_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "If the provider fails to complete the task within the alloted time, the task will be inserted back into the auction with a higher demand 𝑣𝑖. 4.1 Pricing Model We introduce a pricing model as a fair-enough price-determining mechanism to motivate providers and users to participate. We adopt the same pricing model as Zhai et. al. (2018) which considers a ﬁxed allocation matrix. Then, the winning resources of each user will be as follows: resource𝑖= ∑𝐴𝑖,𝑗× ∑𝑠𝑗, 1 ≤𝑖≤𝑚, 1 ≤𝑗≤𝑛 The following two equations are the per unit price for tasks and providers accordingly. 𝑣𝑖 price (task per unit)𝑖= , 1 ≤𝑖≤𝑚, 1 ≤𝑗≤𝑛 ∑𝑥𝑖,𝑗* ∑𝑠𝑗 𝑐𝑗 price (provider per unit)𝑗= , 1 ≤𝑗≤𝑛 ∑𝑠𝑗 4.2 Algorithm We present a polynomial time combinatorial algorithm for Equation 1, which is NP-Hard. Following Zhai et. al. (2018) and Jiang et. al. (2021), our scheduling mechanism invokes a bid density (𝑏𝑑) mechanism to push bids for eﬃcient allocations. Let 𝑀𝑖 represent the compute power of 𝑅𝑖, which will be determined by benchmarking. Let 𝚿 be a constant representing the ability for a service provider to complete a task determined by an algorithm that considers GPU compute power, CPU compute power, etc. and let 𝑀𝑖, 𝑏𝑑𝑡𝑎𝑠𝑘𝑖 , and 𝑏𝑑𝑝𝑟𝑜𝑣𝑖𝑑𝑒𝑟𝑖 be deﬁned as: √ 𝑘 √ 𝑏𝑑𝑡𝑎𝑠𝑘𝑖 = √∑ 𝐶2𝑖,𝑗+ 𝑣2𝑖 ⎷ 𝑗=1 √ 𝑘 √ 𝑏𝑑𝑝𝑟𝑜𝑣𝑖𝑑𝑒𝑟𝑖 = √∑ 1/𝑠2𝑖,𝑗+ ∑ Ψ𝑗𝑖,𝑗 ⎷ 𝑗=1 𝑗=1 Setting the bidding density for service providers in this manner heavily incentivize providers that are able to run WebGPU inference on relatively large models such as Llama and SDXL. The main combinatorial double auction resource allocation algorithm is presented below. As mentioned earlier, this auction mechanism is utilized to determine fair prices for users and service providers. An important qualiﬁcation is that only users are able to inﬂuence the auction by increasing 𝑣𝑖 through bidding more token, which increases the bidding density. However, providers themselves are not able to set the price of their own bid. Instead, it is computed within the platform and a fair price is assigned to the provider. 3", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 3, "chunk_id": "files_oasis_ai_whitepaper_p3_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "Prices(𝑁tasks, 𝑁providers, 𝐵providers, 𝐵tasks): 1 Users and service providers send their bids to the auctioneer. 2 Compute a sorted list of bids of tasks and providers Normalize components of 𝚿 and normalize 𝒔∈𝑀(ℝ), 𝐶 and 3 𝑣 to be in the range (0, 1) 𝐺←⟨𝑏𝑑𝑡𝑎𝑠𝑘𝑖 ⟩ sorted in non-increasing order 𝐻←⟨𝑏𝑑𝑝𝑟𝑜𝑣𝑖𝑑𝑒𝑟𝑖 ⟩ 4 sorted in non-increasing order 5 𝑖= 𝑗←1 6 Flag Price: 7 𝑘←1 8 𝑋∈𝑀𝑚,𝑛(𝟘) // Allocation Matrix 9 𝑃∈𝑀𝑚,𝑛(𝟘) // Price Matrix 10 if (𝐶= 0) ∨(𝑣𝑖< 𝑞𝑖): 11 𝑘 += 1 12 𝑋[𝑖, 𝑗] = 𝑞𝑖 13 𝑃[𝑖, 𝑗] = (Pr(provider per unit)𝑗+ Pr(task per unit)𝑖)/2 14 if ¬(Pr(provider per unit)𝑗≤𝑃[𝑖, 𝑗] ≤Pr(task per unit)𝑖): 15 𝑃[𝑖, 𝑗] = Pr(provider per unit)𝑗 16 𝑣𝑖←𝑣𝑖−𝑞𝑖 17 𝑞𝑖←0 18 If all of the requests of a single user are not satisﬁed: 19 𝑗 += 1 20 GOTO Price 21 If all of the requests of a single user are satisﬁed: 22 Task trade price𝑖= ∑𝑛𝑚=1 𝑃𝑖,𝑚∗𝒔𝒋∗𝐴𝑘,𝑗 23 Device trade price𝑗= ∑𝑛𝑛=1 𝑃𝑛,𝑗∗𝒔𝒋∗𝐴𝑛,𝑗 24 𝑖 += 1 25 GOTO Price 26 If all user requests are satisﬁed: 27 return Task trade price, Device trade price 5 Slashing Mechanism One of the most established security mechanisms are Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARKs). However, zkML is impractical for large machine learning inference tasks (e.g. LLMs, Image Generation, etc.) (see Table 1) [6]. Additionally, zk- SNARKS also have costly memory requirements and high service costs. For instance, the memory consumption for generating an arithmetic circuit in a zk-SNARK for the 7 billion parameter Llama model is in the order of terabytes, if not petabytes [6]. Speciﬁcally, the proof generation 4", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 4, "chunk_id": "files_oasis_ai_whitepaper_p4_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "time is a signiﬁcant limitation, which is why we utilize optimize fault proofs. In this, we design a system to incentivize desired behavior to mitigate potentially invalid results. Optimistic Proof zkML Model size Arbitrary size Small/Limited Proof Type Fraud Proof zk-SNARK (validity proof) Proof Speed Delayed due to challenges Quick/No delays Security crypto-economic security ZK/Cryptographic Table 1: Tradeoﬀs for existing inference veriﬁcation techniques [6] We utilize fault proofs to protect users from misbehaving service providers in a slashing mechanism similar to that of Ethereum’s Proof-of-Stake model. In this optimistic system, users are able to challenge the results of an inference. Additionally, the Oasis.ai system will also randomly initiate challenges. While a challenge is running, the challenged provider is temporarily barred from accepting tasks. We use a method similar to [7] and the interactive bisection scheme in [8] to determine the validity of the challenger’s claim. Since the inference can be represented as a directed acyclic graph (DAG), we can ﬁx the source of randomness to produce deterministic inference processes. Then, a single service provider is assigned to recompute each layer of the node in the topological order of the model DAG. Note that using multiple providers to verify the challenged inference requires full trust since 1-of-𝑛 dishonest parties nulliﬁes the correctness of the proof. If at any point there is a discrepancy between the challenged inference and the recomputed inference, the challenger and service providers are rewarded and the challenged service provider is punished. Punishments result in nefarious providers being removed from the network with prejudice and the invalid transaction being reversed. The user’s task will then be returned to the auction with higher priority. 5", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 5, "chunk_id": "files_oasis_ai_whitepaper_p5_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "References [1] H. Touvron et al., “LLaMA: Open and Eﬃcient Foundation Language Models.” 2023. [2] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with Latent Diﬀusion Models.” 2022. [3] D. Jackson, “Next-generation 3D Graphics on the Web — webkit.org.” 2017. [4] C. Xu and W. Song, “Intelligent Task Allocation for Mobile Crowdsensing With Graph Attention Network and Deep Reinforcement Learning,” IEEE Transactions on Network Science and Engineering, vol. 10, no. 2, pp. 1032–1048, 2023, doi: 10.1109/ TNSE.2022.3226422. [5] Y. Zhai, L. Huang, L. Chen, N. Xiao, and Y. Geng, “COUSTIC: Combinatorial Double auction for Task Assignment in Device-to-Device Clouds.” 2018. [6] K. D. Conway, C. So, X. Yu, and K. Wong, “opML: Optimistic Machine Learning on Blockchain.” Accessed: Mar. 02, 2024. [Online]. Available: http://arxiv.org/abs/2401.17555 [7] S. Bhat et al., “SAKSHI: Decentralized AI Platforms.” 2023. [8] H. Kalodner, S. Goldfeder, X. Chen, S. M. Weinberg, and E. W. Felten, “Arbitrum: Scalable, private smart contracts,” in 27th USENIX Security Symposium (USENIX Security 18), Baltimore, MD: USENIX Association, Aug. 2018, pp. 1353–1370. [Online]. Available: https://www.usenix.org/conference/usenixsecurity18/presentation/kalodner 6", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 6, "chunk_id": "files_oasis_ai_whitepaper_p6_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
