{"text": "Oasis.AI Whitepaper Version 1.0 1 Introduction Over the past decade, machine learning has experienced a rapid growth in popularity, permeating various aspects of daily life. As the demand for compute resources for machine learning tasks continues to surge, existing centralized inference mechanisms are being pushed to their limits. Simultaneously, supply chain shortages have led to year-long back orders on cutting edge hardware while hundreds of thousands of users already have mid-tier to high-end excess compute sitting idle. By connecting this unused capacity via a distributed network, a higher global compute eï¬ƒciency can be achieved. This approach not only ensures cost-eï¬ƒciency but also delivers reliability and performance to a broader spectrum of users. The Oasis.ai machine learning inference platform oï¬€ers distributed Machine Learning as a Service (MLaaS) capabilities. The platform enables individuals to securely execute machine learning inference tasks with an expanding repertoire of popular models such as Llama [1] and Stable Diï¬€usion XL [2], among others. 2 WebGPU-based Machine Learning Despite the clear beneï¬ts of a distributed AI inference platform, the implementation of such a network requires a standardized system for all potential compute nodes to execute. Existing solutions have levered Docker as a means of deploying work units to service providers, oï¬€ering decentralized physical infrastructure networks (DePIN), which presents its own issues. Despite the ï¬‚exibility that this approach oï¬€ers, it comes at the cost of ease of setup and deployment as well opening up additional attack vectors via running unveriï¬ed code. By focusing only on ML inference, rather than generic compute, a custom platform can be implemented in the browser by leveraging targeted APIs. Utilizing existing WebGPU APIs for ML represents a signiï¬cant advancement for large scale, distributed, edge compute by exposing low-level graphics and general purpose computation [3]. Via these APIs, web applications are able to harness the full power of modern GPUs for accelerated computing tasks, which includes machine learning inference. This advancement allows for the ï¬‚exibility to run larger, previously desktop-bound, machine learning model in an existing, proven, browser environment. Moreover, WebGPU oï¬€ers cross-platform support for hardware acceleration, allowing for consistent performance and implementation across a variety of devices. Thus, WebGPU is well-poised as a tool for browser-based MLaaS for making inference accessible for a wide range of applications. The Oasis.ai platform leverages WebGPU technology to enable eï¬ƒcient and scalable distributed machine learning within a web browser environment. Providers are able to exchange access to their WebGPU compute for Oasis.ai token. Speciï¬cally, service providers run inference tasks with WebGPU on behalf of users in exchange for token. The emergence of browser-based machine learning signiï¬es a substantial leap in the access of AI technologies, oï¬€ering low-latency and ï¬‚exible processing directly within existing web applications. 1", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 1, "chunk_id": "files_oasis_ai_whitepaper_p1_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "3 Task Allocation Inference tasks can be allocated to service providers in exchange for tokens. Speciï¬cally, this model involves users exchanging tokenized tasks or requests in the form of digital assets, which can serve as a tradable unit of value for the execution of a speciï¬c machine learning model by service providers. Machine learning service providers can participate in the network by oï¬€ering their computational resources to run inference on an input, and they are rewarded with tokens in exchange. A signiï¬cant challenge in designing a task-allocation system is that determining the optimal task allocation solution is NP-Hard [4]. We implement the diï¬€erential privacy-based combinatorial double auction algorithm proposed by Zhai et al. between users and service providers, and trades are executed when the bid price of a user matches the sellerâ€™s ask price [5]. 4 Combinatorial Double Auction Algorithm Consider a vector of service providers ğ‘¹= âŸ¨ğ‘…1, ğ‘…2, ..., ğ‘…ğ‘›âŸ© and inference tasks ğ‘»= âŸ¨ğ‘‡1, ğ‘‡2, ..., ğ‘‡ğ‘šâŸ©. Additionally, each inference task should be represented as ğ‘‡ğ‘–= âŸ¨ğ¶ğ‘–,1ğ‘¡1, ğ¶ğ‘–,2ğ‘¡2, ...ğ¶ğ‘–,ğ‘˜ğ‘¡ğ‘˜âŸ© where ğ¶ğ‘–,ğ‘— represents the demand for each of the ğ‘˜ inference task types (e.g. each of the machine learning model types). Let ğ‘£ğ‘– represent the adjusted valuation of computing the sub-task ğ‘‡ğ‘–, which reï¬‚ects factors imposed by the demand such as the slashing mechanism discussed in Section 6. Let ğ‘º be the set ğ‘†ğ‘—= (ğ’”ğ’‹, ğ‘¤ğ‘—, ğ‘ğ‘—) where ğ’”ğ’‹ represents the ğ‘—- th resource (e.g. CPU processing power). ğ‘¤ğ‘— represents the maximum number of units of service that device ğ‘— can provide for each resource type ğ‘ ğ‘—,ğ‘– and ğ‘ğ‘— represents the true adjusted valuation of the ğ‘–-th deviceâ€™s cost. We can construct a matrix ğ´âˆˆğ‘€{ğ‘šÃ—ğ‘›}(ğŸ™) such that ğ´ğ‘–,ğ‘—= {1 ğ‘…ğ‘— accepts the ğ‘–-th task 0 else Indeed, âˆ‘ğ‘š ğ‘˜=1 ğ´ğ‘˜,ğ‘—â‰¤1, for 1 â‰¤ğ‘—â‰¤ğ‘› since a model can strictly accept only one model inference. Let the additional cost function be deï¬ned by ğ‘’ğ‘–= ğ‘¡ğ‘–logğ›¼(ğ‘¡ğ‘–) + ğ›½ where ğ›¼, ğ›½âˆˆâ„¤+. Then, a utility function for users can be deï¬ned as priceğ‘–+ ğ‘’ğ‘–) if the ğ‘–-th task wins utilityğ‘¢ğ‘–= {ğ‘£ğ‘–âˆ’(trade 0 else Similarly, the utility function for service providers can be deï¬ned as ğ‘–=1 ğ´ğ‘–,ğ‘— if the ğ‘—-th task wins utilityğ‘ğ‘—= {trade priceğ‘—âˆ’âˆ‘ğ‘› 0 else We must optimize the function max(âˆ‘ğ‘š ğ‘–=1 utilityğ‘¢ğ‘–âˆ’âˆ‘ğ‘›ğ‘—=1 utilityğ‘ğ‘—) such that: ğ‘š âˆ‘ ğ´ğ‘–,ğ‘—â‰¤ğ‘¤ğ‘—For each user device ğ‘–=1 ğ´ğ‘–,ğ‘—âˆˆ[0, 1], âˆ€ğ‘–âˆˆ[1, ğ‘š], ğ‘—âˆˆ[1, ğ‘›] 2", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 2, "chunk_id": "files_oasis_ai_whitepaper_p2_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "If the provider fails to complete the task within the alloted time, the task will be inserted back into the auction with a higher demand ğ‘£ğ‘–. 4.1 Pricing Model We introduce a pricing model as a fair-enough price-determining mechanism to motivate providers and users to participate. We adopt the same pricing model as Zhai et. al. (2018) which considers a ï¬xed allocation matrix. Then, the winning resources of each user will be as follows: resourceğ‘–= âˆ‘ğ´ğ‘–,ğ‘—Ã— âˆ‘ğ‘ ğ‘—, 1 â‰¤ğ‘–â‰¤ğ‘š, 1 â‰¤ğ‘—â‰¤ğ‘› The following two equations are the per unit price for tasks and providers accordingly. ğ‘£ğ‘– price (task per unit)ğ‘–= , 1 â‰¤ğ‘–â‰¤ğ‘š, 1 â‰¤ğ‘—â‰¤ğ‘› âˆ‘ğ‘¥ğ‘–,ğ‘—* âˆ‘ğ‘ ğ‘— ğ‘ğ‘— price (provider per unit)ğ‘—= , 1 â‰¤ğ‘—â‰¤ğ‘› âˆ‘ğ‘ ğ‘— 4.2 Algorithm We present a polynomial time combinatorial algorithm for Equation 1, which is NP-Hard. Following Zhai et. al. (2018) and Jiang et. al. (2021), our scheduling mechanism invokes a bid density (ğ‘ğ‘‘) mechanism to push bids for eï¬ƒcient allocations. Let ğ‘€ğ‘– represent the compute power of ğ‘…ğ‘–, which will be determined by benchmarking. Let ğš¿ be a constant representing the ability for a service provider to complete a task determined by an algorithm that considers GPU compute power, CPU compute power, etc. and let ğ‘€ğ‘–, ğ‘ğ‘‘ğ‘¡ğ‘ğ‘ ğ‘˜ğ‘– , and ğ‘ğ‘‘ğ‘ğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘Ÿğ‘– be deï¬ned as: âˆš ğ‘˜ âˆš ğ‘ğ‘‘ğ‘¡ğ‘ğ‘ ğ‘˜ğ‘– = âˆšâˆ‘ ğ¶2ğ‘–,ğ‘—+ ğ‘£2ğ‘– â· ğ‘—=1 âˆš ğ‘˜ âˆš ğ‘ğ‘‘ğ‘ğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘Ÿğ‘– = âˆšâˆ‘ 1/ğ‘ 2ğ‘–,ğ‘—+ âˆ‘ Î¨ğ‘—ğ‘–,ğ‘— â· ğ‘—=1 ğ‘—=1 Setting the bidding density for service providers in this manner heavily incentivize providers that are able to run WebGPU inference on relatively large models such as Llama and SDXL. The main combinatorial double auction resource allocation algorithm is presented below. As mentioned earlier, this auction mechanism is utilized to determine fair prices for users and service providers. An important qualiï¬cation is that only users are able to inï¬‚uence the auction by increasing ğ‘£ğ‘– through bidding more token, which increases the bidding density. However, providers themselves are not able to set the price of their own bid. Instead, it is computed within the platform and a fair price is assigned to the provider. 3", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 3, "chunk_id": "files_oasis_ai_whitepaper_p3_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "Prices(ğ‘tasks, ğ‘providers, ğµproviders, ğµtasks): 1 Users and service providers send their bids to the auctioneer. 2 Compute a sorted list of bids of tasks and providers Normalize components of ğš¿ and normalize ğ’”âˆˆğ‘€(â„), ğ¶ and 3 ğ‘£ to be in the range (0, 1) ğºâ†âŸ¨ğ‘ğ‘‘ğ‘¡ğ‘ğ‘ ğ‘˜ğ‘– âŸ© sorted in non-increasing order ğ»â†âŸ¨ğ‘ğ‘‘ğ‘ğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘Ÿğ‘– âŸ© 4 sorted in non-increasing order 5 ğ‘–= ğ‘—â†1 6 Flag Price: 7 ğ‘˜â†1 8 ğ‘‹âˆˆğ‘€ğ‘š,ğ‘›(ğŸ˜) // Allocation Matrix 9 ğ‘ƒâˆˆğ‘€ğ‘š,ğ‘›(ğŸ˜) // Price Matrix 10 if (ğ¶= 0) âˆ¨(ğ‘£ğ‘–< ğ‘ğ‘–): 11 ğ‘˜ += 1 12 ğ‘‹[ğ‘–, ğ‘—] = ğ‘ğ‘– 13 ğ‘ƒ[ğ‘–, ğ‘—] = (Pr(provider per unit)ğ‘—+ Pr(task per unit)ğ‘–)/2 14 if Â¬(Pr(provider per unit)ğ‘—â‰¤ğ‘ƒ[ğ‘–, ğ‘—] â‰¤Pr(task per unit)ğ‘–): 15 ğ‘ƒ[ğ‘–, ğ‘—] = Pr(provider per unit)ğ‘— 16 ğ‘£ğ‘–â†ğ‘£ğ‘–âˆ’ğ‘ğ‘– 17 ğ‘ğ‘–â†0 18 If all of the requests of a single user are not satisï¬ed: 19 ğ‘— += 1 20 GOTO Price 21 If all of the requests of a single user are satisï¬ed: 22 Task trade priceğ‘–= âˆ‘ğ‘›ğ‘š=1 ğ‘ƒğ‘–,ğ‘šâˆ—ğ’”ğ’‹âˆ—ğ´ğ‘˜,ğ‘— 23 Device trade priceğ‘—= âˆ‘ğ‘›ğ‘›=1 ğ‘ƒğ‘›,ğ‘—âˆ—ğ’”ğ’‹âˆ—ğ´ğ‘›,ğ‘— 24 ğ‘– += 1 25 GOTO Price 26 If all user requests are satisï¬ed: 27 return Task trade price, Device trade price 5 Slashing Mechanism One of the most established security mechanisms are Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARKs). However, zkML is impractical for large machine learning inference tasks (e.g. LLMs, Image Generation, etc.) (see Table 1) [6]. Additionally, zk- SNARKS also have costly memory requirements and high service costs. For instance, the memory consumption for generating an arithmetic circuit in a zk-SNARK for the 7 billion parameter Llama model is in the order of terabytes, if not petabytes [6]. Speciï¬cally, the proof generation 4", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 4, "chunk_id": "files_oasis_ai_whitepaper_p4_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "time is a signiï¬cant limitation, which is why we utilize optimize fault proofs. In this, we design a system to incentivize desired behavior to mitigate potentially invalid results. Optimistic Proof zkML Model size Arbitrary size Small/Limited Proof Type Fraud Proof zk-SNARK (validity proof) Proof Speed Delayed due to challenges Quick/No delays Security crypto-economic security ZK/Cryptographic Table 1: Tradeoï¬€s for existing inference veriï¬cation techniques [6] We utilize fault proofs to protect users from misbehaving service providers in a slashing mechanism similar to that of Ethereumâ€™s Proof-of-Stake model. In this optimistic system, users are able to challenge the results of an inference. Additionally, the Oasis.ai system will also randomly initiate challenges. While a challenge is running, the challenged provider is temporarily barred from accepting tasks. We use a method similar to [7] and the interactive bisection scheme in [8] to determine the validity of the challengerâ€™s claim. Since the inference can be represented as a directed acyclic graph (DAG), we can ï¬x the source of randomness to produce deterministic inference processes. Then, a single service provider is assigned to recompute each layer of the node in the topological order of the model DAG. Note that using multiple providers to verify the challenged inference requires full trust since 1-of-ğ‘› dishonest parties nulliï¬es the correctness of the proof. If at any point there is a discrepancy between the challenged inference and the recomputed inference, the challenger and service providers are rewarded and the challenged service provider is punished. Punishments result in nefarious providers being removed from the network with prejudice and the invalid transaction being reversed. The userâ€™s task will then be returned to the auction with higher priority. 5", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 5, "chunk_id": "files_oasis_ai_whitepaper_p5_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
{"text": "References [1] H. Touvron et al., â€œLLaMA: Open and Eï¬ƒcient Foundation Language Models.â€ 2023. [2] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, â€œHigh-Resolution Image Synthesis with Latent Diï¬€usion Models.â€ 2022. [3] D. Jackson, â€œNext-generation 3D Graphics on the Web â€” webkit.org.â€ 2017. [4] C. Xu and W. Song, â€œIntelligent Task Allocation for Mobile Crowdsensing With Graph Attention Network and Deep Reinforcement Learning,â€ IEEE Transactions on Network Science and Engineering, vol. 10, no. 2, pp. 1032â€“1048, 2023, doi: 10.1109/ TNSE.2022.3226422. [5] Y. Zhai, L. Huang, L. Chen, N. Xiao, and Y. Geng, â€œCOUSTIC: Combinatorial Double auction for Task Assignment in Device-to-Device Clouds.â€ 2018. [6] K. D. Conway, C. So, X. Yu, and K. Wong, â€œopML: Optimistic Machine Learning on Blockchain.â€ Accessed: Mar. 02, 2024. [Online]. Available: http://arxiv.org/abs/2401.17555 [7] S. Bhat et al., â€œSAKSHI: Decentralized AI Platforms.â€ 2023. [8] H. Kalodner, S. Goldfeder, X. Chen, S. M. Weinberg, and E. W. Felten, â€œArbitrum: Scalable, private smart contracts,â€ in 27th USENIX Security Symposium (USENIX Security 18), Baltimore, MD: USENIX Association, Aug. 2018, pp. 1353â€“1370. [Online]. Available: https://www.usenix.org/conference/usenixsecurity18/presentation/kalodner 6", "metadata": {"source_file": "files_oasis_ai_whitepaper.pdf", "source_full_path": "/home/filadmin/deepseek-RAG/data/downloaded_depin_pdfs/files_oasis_ai_whitepaper.pdf", "page_number": 6, "chunk_id": "files_oasis_ai_whitepaper_p6_c1", "title": "Oasis.AI Whitepaper", "author": "Oasis.AI", "creationDate": "D:20240320004250Z"}}
