# **Deepseek AI: An In-Depth Analysis of Technology, Security, Privacy, and Strategic Implications**

## **Executive Summary**

Deepseek AI, a Chinese artificial intelligence company, has rapidly emerged as a significant player in the global AI landscape, garnering substantial attention for its high-performance large language models (LLMs) and claims of remarkable cost-efficiency.1 Originating from the quantitative hedge fund High-Flyer Quant 2, Deepseek has pursued an ambitious mission focused on advancing AI towards Artificial General Intelligence (AGI).3 This report provides a comprehensive analysis of Deepseek AI, evaluating its technological capabilities, deployment options, security and privacy posture, cost considerations, and the surrounding geopolitical context, aiming to inform strategic decision-making for enterprises considering its technology.

Deepseek's key technical achievements include models demonstrating benchmark performance rivaling top Western counterparts, particularly the DeepSeek-R1 model in reasoning and the DeepSeek-V3 model in general tasks.13 This performance is attributed to innovative architectures like Mixture-of-Experts (MoE) and Multi-head Latent Attention (MLA), designed to enhance computational efficiency.6 Furthermore, Deepseek has explored novel training methodologies, such as employing pure Reinforcement Learning (RL) via Group Relative Policy Optimization (GRPO) to cultivate reasoning abilities in its DeepSeek-R1-Zero model, showcasing emergent behaviors like Chain-of-Thought (CoT) and self-verification.14

However, Deepseek presents a critical dichotomy for potential enterprise users. While its release of powerful open-weight models facilitates local deployment, offering a pathway to enhanced data privacy by keeping information on-premises 31, the company and its services are associated with significant security and privacy risks. Concerns include the company's data collection practices via its API and chat services, the storage of user data in China 33, potential non-compliance with regulations like GDPR, and a history of security incidents, such as a major database exposure 43 and vulnerabilities found in its mobile applications.48 Moreover, the open-weight models themselves, particularly DeepSeek-R1, exhibit alarming susceptibility to jailbreaking 28 and face risks common to open models, such as malicious tampering and data poisoning.57

The narrative of Deepseek's cost-efficiency, particularly the widely cited claim of training DeepSeek-V3 for approximately $5.6 million 1, requires careful scrutiny. While indicative of potential architectural and training optimizations, this figure likely excludes substantial R\&D, hardware, and operational costs, with total investment potentially reaching hundreds of millions or even billions.8 Crucially for enterprises, this claimed *training* efficiency does not eliminate the significant hardware expenditure required to *run* these large models locally.82 Furthermore, attempting local *training* or substantial fine-tuning of these models remains practically infeasible for most organizations due to the immense resource requirements.94

Deepseek operates within a complex geopolitical landscape marked by escalating US-China technological competition. A U.S. House Select Committee report in April 2025 labeled Deepseek a national security threat, citing concerns over data privacy, censorship, potential intellectual property theft, and the alleged use of export-controlled chips.1 These findings have led to calls for stricter U.S. government actions, including enhanced export controls, and add a significant layer of strategic risk to engaging with Deepseek technology.98

Overall, enterprises evaluating Deepseek must conduct a cautious and thorough assessment, balancing the potential technological advantages and cost efficiencies against substantial and well-documented security, privacy, compliance, and geopolitical risks. If utilization is pursued, prioritizing the local deployment of open-weight models (particularly distilled versions for feasibility) coupled with robust internal security controls appears to be the most prudent approach, while avoiding the company's direct API and chat services for any sensitive applications is strongly recommended.

## **I. Introduction to Deepseek AI**

### **A. Company Background: Origins from High-Flyer, Mission, and AGI Focus**

Deepseek AI (officially Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.) is a Chinese artificial intelligence research company established in July 2023\.2 It emerged as a spin-off from High-Flyer Quant, a prominent Chinese quantitative hedge fund.2 Both Deepseek and High-Flyer are led by CEO Liang Wenfeng, who co-founded the hedge fund in 2016 after graduating from Zhejiang University.1

High-Flyer's background is crucial to understanding Deepseek's trajectory. The hedge fund specialized in developing and deploying AI-driven algorithms for quantitative trading, transitioning exclusively to AI-based strategies by 2021\.2 This focus necessitated significant investment in computational infrastructure. High-Flyer reportedly began building its first GPU cluster ("Fire-Flyer") in 2019 and started accumulating large quantities of Nvidia GPUs as early as 2021, notably acquiring a reported 10,000 Nvidia A100 GPUs before the U.S. imposed restrictions on their sale to China.2 A second, larger cluster ("Fire-Flyer 2") began construction in 2021\.6 This early and substantial investment in hardware provided Deepseek with a strong foundation in terms of computing resources and expertise in optimizing large-scale computations.9

In April 2023, High-Flyer announced the launch of an AGI research lab focused on AI tools unrelated to its financial business, which was subsequently spun off as the independent company Deepseek AI in July 2023, with High-Flyer as its principal investor.6 Deepseek's stated mission is to push the boundaries of AI, create impactful solutions, and make advanced AI technology accessible and ethical.58 A core, long-term objective is the pursuit of Artificial General Intelligence (AGI) – machine intelligence capable of human-like understanding and learning.3

Deepseek's strategic approach involves a strong focus on fundamental research rather than immediate commercialization, a posture that may also help it navigate certain Chinese AI regulations aimed at consumer products.6 The company emphasizes open-weight model releases under permissive licenses like MIT (though specific model licenses apply).2 Its hiring strategy targets young, skilled graduates from top Chinese universities, including those from non-computer science fields like poetry and mathematics, to broaden model expertise.6 Furthermore, Deepseek has focused on developing novel model architectures designed to perform effectively even with potential constraints on accessing the latest AI chips due to export controls.6

The origin of Deepseek from a quantitative trading firm is noteworthy. The demanding computational environment of high-frequency trading cultivates expertise in optimizing algorithms and infrastructure, skills directly applicable to the challenges of training large-scale LLMs.9 High-Flyer's financial success likely provided the substantial, patient capital needed for a long-term, research-intensive AGI mission, bypassing the typical pressures of venture capital funding cycles that often demand quicker commercial returns.6 This background may also explain the company's emphasis on computational and cost efficiency as a key differentiator, reflecting the resource optimization mindset inherent in quantitative finance. However, this finance-centric origin might also mean that initial development priorities placed less emphasis on aspects like user interface polish, robust security practices, or navigating complex global compliance landscapes compared to established consumer-facing technology companies. This potential imbalance could offer context for the significant security and privacy issues later identified in Deepseek's services and applications.28

### **B. Significance in the AI Landscape: Performance Benchmarks and Claimed Cost-Efficiency**

Deepseek AI rapidly ascended from relative obscurity to become a major talking point in the global AI landscape, challenging established Western players like OpenAI, Google, and Anthropic.3 Its emergence prompted strong reactions, including comparisons to the "Sputnik moment" for the AI industry, signifying a perceived leap forward by a geopolitical competitor.2

This significance is largely driven by Deepseek's impressive performance on various industry-standard benchmarks, often achieving parity or even exceeding results from well-regarded models. Key examples include:

* **DeepSeek LLM 67B:** Surpassed Meta's Llama-2 70B on multiple benchmarks, particularly in coding, mathematics, and reasoning.120  
* **DeepSeek-V2:** Competed favorably with models like GPT-4-Turbo, ranking highly on leaderboards like AlignBench despite having fewer activated parameters.19  
* **DeepSeek-V3:** Outperformed Llama 3.1 405B and GPT-4o on several key benchmarks, showing exceptional strength in coding and math.16  
* **DeepSeek-R1:** Demonstrated reasoning capabilities comparable to OpenAI's o1 and o3-mini models, particularly on challenging math and reasoning benchmarks like AIME 2024, MATH-500, and Codeforces.13 Performance was also strong on open-ended evaluations like Arena-Hard and AlpacaEval 2.0.14

Equally disruptive has been Deepseek's claim of achieving this high performance with remarkable cost-efficiency, particularly in model training.1 The most cited figure is the assertion that DeepSeek-V3 was pre-trained for approximately $5.6 million in compute costs, utilizing 2,048 Nvidia H800 GPUs over roughly two months.7 This contrasts sharply with estimates for models like GPT-4, often placed at $100 million or more.4 Similarly, DeepSeek-V2 was reported to save 42.5% in training costs compared to the earlier DeepSeek 67B model.9 This efficiency is partly attributed to their architectural choices and potentially the use of less advanced but optimized hardware (like the H800/A100 chips available before stricter US export controls).5

The combination of high benchmark scores, claimed low training costs, and the company's commitment to releasing powerful open-weight models 2 underpins Deepseek's disruptive potential. While high performance is expected from leading AI labs, and low cost might suggest compromised quality 81, and open-source models often trail proprietary ones, Deepseek's asserted simultaneous achievement across all three dimensions challenges the prevailing narrative.3 It suggests that state-of-the-art AI might be achievable without the massive capital investments and proprietary datasets previously thought necessary, potentially democratizing access to powerful AI capabilities.1 However, this significance is contested due to ambiguities surrounding the true total cost of development 78 and overshadowed by significant security, privacy, and geopolitical concerns.1

## **II. Deepseek's Model Ecosystem and Architectural Innovations**

Deepseek AI has rapidly developed and released a diverse portfolio of large language models, targeting various capabilities and employing novel architectural designs aimed at enhancing performance and efficiency.

### **A. Key Model Families: DeepSeek LLM, DeepSeek Coder, DeepSeek-R1 Series, DeepSeek-V2/V3**

Deepseek's model releases have followed a rapid cadence since late 2023 122, demonstrating significant R\&D momentum:

1. **DeepSeek LLM (November 2023):** This marked Deepseek's initial foray into general-purpose LLMs, featuring 7B and 67B parameter models. Trained on a dataset of 2 trillion tokens (English and Chinese), the 67B model was positioned as competitive with Meta's Llama 2 70B, particularly in reasoning, coding, math, and Chinese comprehension.120  
2. **DeepSeek Coder (November 2023):** Released shortly after the general LLMs, this family focused specifically on code generation and understanding. Models ranged from 1.3B to 33B parameters and were noted for their commercial-grade capabilities.122  
3. **DeepSeek-V2 (May 2024):** A significant architectural evolution, DeepSeek-V2 introduced the Mixture-of-Experts (MoE) and Multi-head Latent Attention (MLA) designs. The main model featured 236B total parameters but activated only 21B per token, supporting a 128K context length and trained on 8.1T tokens.19 A smaller "Lite" version with 16B total / 2.4B active parameters was also released.24  
4. **DeepSeek Coder V2 (July 2024):** An upgrade to the coder series, this MoE model (also 236B total parameters) expanded support to 338 programming languages and maintained the 128K context window, targeting complex coding challenges.124  
5. **DeepSeek-V3 (December 2024):** Deepseek's flagship general-purpose MoE model, scaling up to 671B total parameters (activating 37B per token) with a 128K context length.16 It incorporated refinements to MoE/MLA and introduced FP8 mixed-precision training, trained on an extensive 14.8T token dataset. Performance claims positioned it as outperforming Llama 3.1 405B and rivaling GPT-4o.16  
6. **DeepSeek-R1 / R1-Zero (January 2025):** These models, based on the DeepSeek-V3 architecture, specifically target advanced reasoning capabilities.14 R1-Zero was trained using pure Reinforcement Learning (RL) with Group Relative Policy Optimization (GRPO), while R1 incorporated SFT stages for refinement. Deepseek also released several "distilled" versions of R1, transferring reasoning capabilities to smaller, dense models (1.5B to 70B parameters) based on popular open-source architectures like Qwen and Llama.13  
7. **DeepSeek-VL (Vision-Language):** Deepseek has also developed multimodal models capable of processing both text and images, such as DeepSeek-VL and VL2.122

This rapid succession of releases across general-purpose, coding, reasoning, and vision domains within approximately one year signifies a highly dynamic and capable R\&D operation. It suggests an ability to quickly iterate on model architectures and training techniques, potentially leveraging their substantial compute resources effectively. This pace challenges perceptions of Chinese AI development lagging significantly behind the West and indicates a strategic push to establish leadership across multiple AI capability dimensions.8

### **B. Core Architectural Features: Mixture-of-Experts (MoE) and Multi-head Latent Attention (MLA)**

Two key architectural innovations underpin the performance and claimed efficiency of Deepseek's more recent models (V2, V3, R1): Mixture-of-Experts (MoE) and Multi-head Latent Attention (MLA).

Mixture-of-Experts (MoE):  
The MoE architecture addresses the computational cost of scaling dense transformer models.3 Instead of activating all model parameters for every input token, MoE models employ sparsity. They contain multiple "expert" sub-networks (typically within the Feed-Forward Network layers) and a routing mechanism (gating network) that selects only a small subset of these experts to process each token.23  
Deepseek's implementation, termed "DeepSeekMoE," utilizes a fine-grained expert structure. For example, in DeepSeek-V3, each MoE layer contains 256 specialized "routed" experts and 1 shared expert that processes all tokens. The gating network selects 8 of the 256 routed experts for each token.22 This sparse activation brings several advantages:

* **Economical Training:** Activating only a fraction of the parameters per token significantly reduces the computational load (FLOPs) required during training, contributing to the claimed cost savings.19  
* **Efficient Inference:** Similarly, inference requires less computation per token compared to a dense model of the same total size, potentially leading to faster response times and lower energy consumption.16  
* **Scalability:** MoE allows models to have a very large total parameter count (e.g., 671B in V3) – enabling greater knowledge capacity – while keeping the number of parameters activated per token relatively small (37B in V3), making inference more manageable.16

Deepseek-V3 further innovated by introducing an auxiliary-loss-free load balancing strategy. Traditional MoE training often uses an auxiliary loss term to encourage the router to distribute tokens evenly across experts, preventing some experts from being over-utilized while others are idle. However, this auxiliary loss can sometimes negatively impact model performance. Deepseek-V3's approach dynamically adjusts a bias term for each expert during training to encourage balance, without directly incorporating a balancing term into the main loss function, aiming to minimize performance degradation.20

Multi-head Latent Attention (MLA):  
Introduced in DeepSeek-V2 and refined in V3, MLA tackles a major bottleneck in transformer inference: the size of the Key-Value (KV) cache.19 In standard attention mechanisms (like Multi-Head Attention \- MHA, or Grouped-Query Attention \- GQA), the keys and values for all previous tokens in the sequence must be stored in memory (the KV cache) to compute the attention scores for the current token. As the sequence length grows, this cache consumes significant memory bandwidth, limiting inference speed and the maximum context length a model can handle efficiently.21  
MLA addresses this by introducing a compression step.19 It uses learnable projection matrices to jointly compress the key and value vectors for each token into a much lower-dimensional "latent" vector *before* they are stored in the KV cache. During attention computation, the query vector attends to these compressed latent vectors. This significantly reduces the memory footprint of the KV cache. DeepSeek-V2 claimed a 93.3% reduction in KV cache size compared to the DeepSeek 67B model (which likely used a standard attention mechanism).19 MLA also requires modifications to how positional information is encoded, leading Deepseek to use a "decoupled" Rotary Position Embedding (RoPE) approach where RoPE is applied only to certain dimensions compatible with the compression.21

The benefits of MLA include:

* **Increased Throughput:** By reducing the memory bandwidth bottleneck, MLA allows for faster token generation speeds. DeepSeek-V2 claimed a 5.76x boost in maximum generation throughput compared to DeepSeek 67B.19  
* **Longer Context Handling:** The reduced memory footprint makes it more feasible to support very long context windows (e.g., 128K tokens in V2 and V3) during inference.16  
* **Overall Inference Efficiency:** MLA contributes significantly to the overall efficiency of Deepseek's models during deployment.19

The combination of DeepSeekMoE and MLA represents a concerted effort by Deepseek to optimize both training and inference efficiency through architectural design. These innovations are central to their ability to train extremely large models with purportedly lower compute budgets and achieve high performance during deployment. This focus on architectural efficiency, possibly driven by the need to maximize performance from available hardware under export controls 5, marks a significant technical contribution to the field and offers a pathway to scaling LLMs that relies less on sheer brute-force computation.3 The open-sourcing of models incorporating these architectures allows the broader research community to study, adapt, and potentially improve upon these techniques.21

## **III. Advanced Reasoning Capabilities: Chain-of-Thought and Reinforcement Learning**

A key differentiator for Deepseek, particularly highlighted by the DeepSeek-R1 series, is its focus on enhancing the reasoning abilities of LLMs. This was achieved through a combination of Chain-of-Thought prompting techniques and innovative applications of Reinforcement Learning.

### **A. Chain-of-Thought (CoT) Implementation in DeepSeek-R1**

Chain-of-Thought (CoT) reasoning is a technique that prompts LLMs to break down complex problems into a series of intermediate steps, explicitly generating the "thought process" leading to a final answer, much like humans showing their work.27 This approach has been shown to significantly improve performance on tasks requiring multi-step logic, mathematics, and planning.

Deepseek heavily utilized and incentivized CoT in the training of its R1 models. Both DeepSeek-R1-Zero and the refined DeepSeek-R1 were trained to structure their responses by first generating a reasoning process enclosed within specific tags (\<think\> and \</think\>) before outputting the final answer in \<answer\> tags.14 This explicit formatting was enforced through format-based rewards during the RL phase for R1-Zero 27 and likely incorporated into the SFT data for R1.

A notable observation during the training of R1-Zero was the model's emergent behavior of allocating more "thinking time" – generating longer and more detailed CoT sequences – for more complex problems.14 This suggests the model learned to dynamically adjust its computational effort based on task difficulty, a desirable trait for efficient problem-solving. However, while beneficial for reasoning, the explicit exposure of the CoT in R1's outputs has been identified as a potential security vulnerability, potentially revealing internal model logic or sensitive information embedded in the reasoning process, thus increasing the attack surface for prompt injection or data leakage attacks.34

### **B. Role of Reinforcement Learning: Group Relative Policy Optimization (GRPO)**

Reinforcement Learning (RL) played a pivotal role in developing the reasoning capabilities of the DeepSeek-R1 series, particularly R1-Zero.13 DeepSeek-R1-Zero was a groundbreaking experiment where RL was applied *directly* to the DeepSeek-V3-Base model *without* any prior Supervised Fine-Tuning (SFT) focused on reasoning tasks. The goal was to explore whether complex reasoning abilities could emerge purely through RL-driven self-evolution.14

To manage the significant computational cost typically associated with RL for large models (which often involves training both a policy model and an equally large critic model), Deepseek employed a specific RL algorithm called **Group Relative Policy Optimization (GRPO)**.27 GRPO avoids the need for a separate critic model. Instead, for each input prompt, it samples multiple potential outputs (a "group") from the current policy model. It then estimates a baseline reward based on the scores of the outputs within that group. The policy is optimized by maximizing an objective function that considers the probability ratio of generating an output under the new versus the old policy (clipped to prevent large deviations), weighted by an advantage term calculated relative to the group's baseline reward. A Kullback-Leibler (KL) divergence penalty term is included to regularize the policy update, keeping it close to a reference policy.27

The reward signal used to train R1-Zero via GRPO was primarily rule-based. For tasks with verifiable answers (like math problems or coding challenges), an **accuracy reward** was given if the final answer was correct. For coding, compiler feedback could be used.27 A **format reward** incentivized the model to adhere to the \<think\>...\</think\>\<answer\>...\</answer\> structure.14

While R1-Zero demonstrated the power of pure RL, it suffered from readability issues and language mixing.26 The refined DeepSeek-R1 model addressed this through a multi-stage training pipeline 22:

1. **Cold Start SFT:** The DeepSeek-V3-Base model was first fine-tuned on a small dataset (thousands) of high-quality, human-annotated long CoT examples. This "cold start" aimed to improve readability and provide a better initialization for RL.27  
2. **Reasoning-oriented RL:** GRPO was applied to the cold-started model, focusing on reasoning tasks. A language consistency reward was added to the accuracy/format rewards to discourage language mixing.27  
3. **Rejection Sampling & SFT:** The RL-trained model was used to generate a larger SFT dataset (\~800k samples) via rejection sampling, keeping only high-quality reasoning outputs. This dataset was augmented with non-reasoning data (writing, QA, etc.) generated using DeepSeek-V3. The base model was then fine-tuned again on this combined dataset.22  
4. **Final RL for Alignment:** A final RL stage aligned the model further with human preferences for helpfulness and harmlessness, using a mix of rule-based and model-based rewards across diverse tasks.22

### **C. Emergent Reasoning Behaviors: Self-Verification and Reflection**

One of the most significant outcomes of Deepseek's RL-driven training, especially evident in the R1-Zero development, was the emergence of complex reasoning behaviors that were not explicitly programmed into the model or the reward function.14 These behaviors arose organically as the model optimized for solving problems effectively during the RL process.

Key emergent behaviors included:

* **Self-Verification:** The model demonstrated the ability to internally check its work. It learned to revisit and re-evaluate intermediate steps within its generated CoT to ensure their correctness before proceeding.14 This is analogous to a human double-checking their calculations.  
* **Reflection:** Beyond simple verification, the model exhibited signs of reflection, analyzing its own reasoning process and sometimes considering alternative approaches to solve a problem.14 A particularly interesting example cited in the research was an "Aha moment" where an intermediate version of the model, when tackling a math problem, initially followed one path, then appeared to recognize an issue or a better strategy, and backtracked to pursue a different line of reasoning, ultimately allocating more "thinking time" or computational steps to reach the correct solution.15

These emergent capabilities are closely linked to the model's tendency, developed during RL, to generate longer and more detailed CoT sequences for complex tasks.27 By being rewarded for reaching correct final answers, the RL process implicitly incentivized the model to develop internal strategies – like longer chains of thought incorporating self-checks and reflection – that improved its problem-solving success rate.

The success of Deepseek's RL approach, particularly GRPO, in eliciting these sophisticated, human-like reasoning patterns is a significant finding. It validates RL not just as a tool for improving benchmark scores but as a mechanism for enhancing the intrinsic reasoning and problem-solving processes within LLMs. The R1-Zero experiment demonstrated that complex reasoning could emerge largely from self-evolution driven by RL, potentially reducing the burden of creating massive, explicitly annotated SFT datasets for teaching complex reasoning skills.27 The subsequent refinement in the DeepSeek-R1 pipeline, incorporating SFT to address RL's weaknesses in output quality 26, represents a pragmatic synergy, leveraging RL for core reasoning development and SFT for polishing and alignment.

## **IV. Local Deployment Strategies for Enhanced Privacy**

For enterprises concerned about data privacy, particularly given the issues surrounding Deepseek's cloud services, deploying models locally (on-premises) presents a compelling alternative. Running open-weight models within an organization's own infrastructure ensures that sensitive data, prompts, and generated outputs do not leave the controlled environment.31

### **A. Frameworks for Local Inference: Ollama and vLLM Setup**

The feasibility of local deployment is significantly aided by the availability of frameworks designed to simplify the process of running LLMs on local hardware. Two prominent options frequently mentioned in relation to Deepseek models are Ollama and vLLM.

* **Ollama:** This framework is widely recognized for its user-friendliness and ease of setup.31 It allows users to download and run various open-source LLMs, including Deepseek's distilled R1 models and potentially other families 140, with simple command-line instructions on macOS, Windows (via WSL), and Linux.  
  * *Setup:* Typically involves downloading and installing the Ollama application.32 Models can then be pulled and run using commands like ollama run deepseek-r1:7b or ollama pull deepseek-r1:1.5b.32 Ollama runs a local server (usually at http://127.0.0.1:11434 or http://localhost:11434) 31 that can be interacted with directly via the command line or through compatible graphical user interfaces (GUIs) like Chatbox 32 or Open WebUI.84 OpenWebUI can often be run easily via Docker.84  
* **vLLM:** Positioned as a high-performance LLM inference and serving engine, vLLM is often recommended for scenarios requiring higher throughput, lower latency, or more advanced deployment configurations.23 It implements optimizations like PagedAttention to improve efficiency.  
  * *Setup:* Installation typically involves using pip (pip install vllm) 141, but may require specific versions of dependencies like PyTorch and CUDA, especially if building from source or targeting specific hardware (e.g., Kunpeng CPUs or specific CUDA versions).61 vLLM can be run as an OpenAI-compatible API server using commands like python \-m vllm.entrypoints.api\_server \--model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \--port 8000 or vllm serve "model\_name".61 Configuration often involves setting parameters like \--tensor-parallel-size (for multi-GPU inference), \--max-model-len, and potentially \--trust-remote-code depending on the model.61  
* **Other Frameworks:** Deepseek's documentation also mentions support or compatibility with other inference engines, particularly for the V3 model, including SGLang (noted for MLA and FP8 support) 16, LMDeploy 23, NVIDIA's TensorRT-LLM 23, and Hugging Face's Text Generation Inference (TGI).122 SGLang and vLLM appear to be frequently recommended options.16

The existence of accessible frameworks like Ollama significantly lowers the entry barrier for experimenting with local deployment, while performance-oriented engines like vLLM and SGLang provide pathways for scaling to enterprise-level workloads.31 Deepseek's apparent engagement with these communities 127 suggests an acknowledgment and potential encouragement of local deployment strategies. This ecosystem facilitates leveraging the privacy benefits of on-premises inference. However, users should be aware of potential setup complexities, especially with performance-focused frameworks or non-standard hardware configurations.61

### **B. Hardware Requirements for Local Deployment (Distilled vs. Full Models)**

The primary determinant of hardware requirements for local LLM deployment is the model size (number of parameters) and the precision used for inference (e.g., FP16, BF16, or quantized formats like INT8, INT4).82 Larger models require substantially more GPU Video RAM (VRAM) or system RAM if running on CPU.

Deepseek Distilled R1 Models:  
These models, ranging from 1.5B to 70B parameters, are significantly more accessible for local deployment than the full 671B model.

* **1.5B (Qwen):** Requires minimal resources, estimated around \~0.7 GB VRAM 83 or \~3GB system RAM.82 Runs comfortably on CPU 82 or entry-level GPUs with at least 6GB VRAM.93  
* **7B/8B (Qwen/Llama):** Generally require 8GB to 16GB of VRAM for reasonable performance.82 GPUs like Nvidia RTX 3060 (12GB) or RTX 3070 (8GB) are often cited as suitable.83 Quantized versions (e.g., 4-bit) can run in as low as 4-6GB VRAM.82 CPU inference is possible with 16GB+ system RAM but will be slow.82 VRAM estimate: \~3.3-3.7GB.83  
* **14B (Qwen):** Needs approximately 16GB to 24GB VRAM.82 GPUs like RTX 3080 (10GB+), RTX 3090 (24GB), or RTX 4090 (24GB) are recommended.83 Can run on an 8GB RTX 3070 but performance may be slow.83 VRAM estimate: \~6.5GB.83  
* **32B (Qwen):** Requires roughly 24GB to 48GB VRAM.82 An RTX 4090 (24GB) is a common recommendation.83 Can run (slowly) on systems with 32GB+ RAM.137 VRAM estimate: \~14.9GB.83  
* **70B (Llama):** Demands 48GB+ VRAM.82 Often requires a multi-GPU setup, such as 2x RTX 4090 (24GB each) 83 or workstation/server GPUs like the A100.83 VRAM estimate: \~32.7GB.83 Can potentially run on high-memory unified systems like specific Mac configurations.85

Deepseek Full R1/V3 Models (671B total / 37B active):  
Running the full-size models locally presents a significant hardware challenge.

* **VRAM Requirements:** Full precision (FP16/BF16) inference requires an estimated 1.3TB to 1.5TB of VRAM 83, far exceeding any single GPU. Even aggressive 4-bit quantization requires approximately 386GB of VRAM/RAM.91  
* **GPU Deployment:** Necessitates large multi-GPU clusters. Estimates suggest configurations like 16x Nvidia A100 80GB 83 or potentially 6-8x Nvidia H100 80GB for quantized versions.91 Consumer setups, even with multiple high-end cards (e.g., 4x 4090 providing 96GB VRAM), are generally insufficient without complex offloading techniques.90  
* **CPU/RAM Deployment:** Inference using system RAM is technically feasible but requires very large amounts of RAM (256GB minimum, often 512GB-1TB+ recommended) and powerful server-grade CPUs (e.g., AMD EPYC) with high memory bandwidth (e.g., 12-channel DDR5).85 Performance is significantly slower than GPU inference (e.g., 5-16 tokens/second reported 85), suitable mainly for non-real-time or batch processing tasks.

**Hardware Requirements Summary Table:**

| Model Variant | Parameters (Total/Active) | Precision | Est. VRAM (GPU) | Est. RAM (CPU) | Recommended GPU(s) |
| :---- | :---- | :---- | :---- | :---- | :---- |
| R1-Distill-Qwen-1.5B | 1.5B / 1.5B | FP16 | \~1-3 GB | 6 GB+ | Entry-level GPU (e.g., RTX 3060\) or CPU |
| R1-Distill-Qwen/Llama-7B/8B | 7B/8B / 7B/8B | FP16 | \~16 GB | 32 GB+ | RTX 3060 (12GB), RTX 3070/3080 (8GB+) |
| R1-Distill-Qwen/Llama-7B/8B | 7B/8B / 7B/8B | 4-bit | \~4-6 GB | 16 GB+ | Most modern GPUs w/ 8GB+ VRAM |
| R1-Distill-Qwen-14B | 14B / 14B | FP16 | \~32 GB | 64 GB+ | RTX 3080 (10GB+), RTX 3090/4090 (24GB) |
| R1-Distill-Qwen-14B | 14B / 14B | 4-bit | \~9 GB | 32 GB+ | RTX 3060 (12GB), RTX 3080 (10GB+) |
| R1-Distill-Qwen-32B | 32B / 32B | FP16 | \~64 GB | 128 GB+ | RTX 4090 (24GB), A100 (40GB+) |
| R1-Distill-Qwen-32B | 32B / 32B | 4-bit | \~15-20 GB | 64 GB+ | RTX 3090/4090 (24GB) |
| R1-Distill-Llama-70B | 70B / 70B | FP16 | \~154 GB | 256 GB+ | 2x RTX 4090 (24GB), A100 (80GB) |
| R1-Distill-Llama-70B | 70B / 70B | 4-bit | \~38 GB | 128 GB+ | RTX 4090 (24GB x2), A100 (40GB+) |
| DeepSeek V3/R1 (Full) | 671B / 37B | FP16 | \~1.3-1.5 TB | N/A | Multi-GPU Cluster (e.g., 16x A100 80GB) |
| DeepSeek V3/R1 (Full) | 671B / 37B | 4-bit | \~386 GB | 512 GB \- 1TB+ | Multi-GPU Cluster (e.g., 6x H100 80GB) or High-RAM CPU |

*Note: VRAM/RAM requirements are approximate and vary with quantization, batch size, context length, and inference framework. GPU recommendations assume typical inference workloads.*

The significant resource gap between distilled and full models is clear. While distilled versions offer a pragmatic route to local deployment on accessible hardware, leveraging the full 671B parameter models necessitates substantial investment in specialized server infrastructure or large GPU clusters, placing it beyond the reach of most enterprises without dedicated AI compute budgets.83 This presents a trade-off: prioritize privacy with manageable hardware using distilled models (accepting potentially lower capability) or pursue maximum capability via the full model, which likely requires either massive local investment or using potentially risky cloud/API services.

### **C. Privacy Advantages of On-Premises Deployment**

The primary motivation for enterprises to pursue local deployment of Deepseek models is to enhance data privacy and maintain control over sensitive information.31 By running the models entirely within their own IT environment, organizations can prevent proprietary data, customer information, internal communications, or any other sensitive inputs/outputs from being transmitted to external servers.

This directly contrasts with using Deepseek's cloud-based API, Chat platform, or mobile applications. As detailed in Deepseek's privacy policies and confirmed by security analyses, using these services involves sending user data – including prompts, chat history, device information, and potentially more – to servers located in the People's Republic of China.33 Furthermore, Deepseek reserves the right to use this data for service improvement, which typically includes model training, often without a clear opt-out mechanism.35

Local deployment effectively mitigates these specific risks:

* **Data Residency:** Information remains within the organization's chosen geographical and infrastructure boundaries, avoiding storage in China.  
* **Data Control:** The organization retains full control over its data; Deepseek has no access to the inputs or outputs processed locally.  
* **Training Data:** Locally processed data is not used to train Deepseek's future models.  
* **Compliance:** Facilitates compliance with data protection regulations like GDPR and CCPA, which impose strict rules on cross-border data transfers, especially to jurisdictions without adequacy decisions like China.33  
* **Government Access:** Reduces the risk of data being accessed by foreign government authorities under local laws (e.g., China's National Security Law and Cybersecurity Law).33

Security experts and institutions have consequently recommended using Deepseek models via local deployment or through trusted third-party platforms (like AWS Bedrock or Perplexity, which host the models in their own controlled environments) rather than interacting directly with Deepseek's services, especially when dealing with sensitive or regulated data.33 Local deployment thus provides a crucial technical mechanism to decouple the use of Deepseek's *model technology* from the privacy risks associated with its *service infrastructure and data handling policies*. It allows organizations to leverage the potential benefits of the open-weight models while maintaining control over their data environment. However, it is critical to remember that this addresses data privacy during *inference* but does not mitigate security risks inherent *within* the model itself, such as biases or vulnerabilities to jailbreaking, which are discussed next.

## **V. Security Posture Analysis: Risks and Mitigation**

While local deployment can address data privacy concerns related to Deepseek's services, a comprehensive security assessment must also evaluate the risks associated with the services themselves (for API users) and the inherent vulnerabilities within the models, regardless of deployment method.

### **A. API and Service-Related Risks**

Enterprises considering using Deepseek's API, Chat platform, or mobile applications face significant risks stemming from the company's data handling practices, compliance challenges, and documented security incidents.

Data Privacy Policy Analysis:  
Deepseek's Privacy Policy 41 outlines extensive data collection practices when users interact with their services:

* **User-Provided Information:** Account details (email, phone number, date of birth, username, password), prompts, text inputs, uploaded files, feedback, and chat history.36  
* **Automatically Collected Information:** Device details (model, OS, identifiers), network information (IP address), system language, usage logs (features used, actions taken), approximate location (via IP), cookies, and payment transaction information.36 Notably, collection of "keystroke patterns or rhythms" has also been mentioned, raising concerns about behavioral biometric tracking.33  
* **Data Usage:** Data is used for service provision, maintenance, administration, enforcing terms, user support, development, and improvement.36 Crucially, this includes using inputs and outputs for *model training*.35 While users can delete chat history 36, a clear mechanism to opt-out of data usage for training appears absent.148  
* **Data Storage Location:** The policy explicitly states that collected information is stored on **secure servers located in the People's Republic of China**.33  
* **Data Sharing:** Information may be shared within Deepseek's corporate group, with third parties during corporate transactions (mergers, acquisitions), and potentially with advertising and analytics partners.33 The policy also mentions receiving data *from* partners about user activities on other sites/apps to help "match" users.33 Integrations with tracking tools from Chinese tech giants (ByteDance, Baidu, Tencent) and hardcoded links to China Mobile (a state-owned telecom designated as a Chinese military company by the US) have been reported.62  
* **User Rights & Retention:** Users generally have rights to access, correct, or delete their data, and withdraw consent, subject to jurisdictional laws.36 However, the effectiveness and clarity of these rights have been questioned.38 Data may be retained even after account deletion if required by law or for "legitimate business interests," without a defined retention period specified.38

Compliance Considerations (GDPR, CCPA):  
The storage of personal data, particularly from EU or US residents, on servers in China raises significant compliance challenges under regulations like GDPR and CCPA.33 These regulations impose strict conditions on transferring personal data outside their jurisdictions, especially to countries lacking an "adequacy decision" (which China does not have from the EU or under CCPA frameworks). Deepseek's privacy policy states transfers will comply with applicable laws but lacks specifics on the safeguards used (e.g., Standard Contractual Clauses).38 Furthermore, Chinese laws like the National Security Law and Cybersecurity Law can potentially compel companies operating in China to provide data access to state authorities, creating a direct conflict with the principles and restrictions of GDPR/CCPA regarding government access to personal data.33  
Known Security Incidents:  
Deepseek's services have been marred by significant, publicly disclosed security failures:

* **Database Exposure (January/February 2025):** Wiz Research discovered a publicly accessible ClickHouse database associated with Deepseek's services (hosted at oauth2callback.deepseek.com:9000 and dev.deepseek.com:9000) that required no authentication.43 This exposed over a million log entries containing sensitive data like user chat histories (plaintext), API keys, backend system details, and operational metadata. The exposure allowed full database control and was reportedly found easily ("within minutes") due to the lack of basic security controls.43 Deepseek secured the database promptly after being notified, although Wiz noted difficulties in finding a proper channel for responsible disclosure.43  
* **Mobile App Vulnerabilities (February 2025):** NowSecure and SecurityScorecard reported multiple critical flaws in Deepseek's mobile apps (primarily iOS, but likely affecting Android too).47 These included:  
  * Sending sensitive registration and device data over the internet unencrypted (due to disabling Apple's App Transport Security \- ATS).50  
  * Using weak and deprecated encryption algorithms (3DES) with hardcoded keys and reused or null Initialization Vectors (IVs) for data that *was* encrypted.49  
  * Insecurely storing sensitive data like usernames, passwords, and encryption keys in cached databases on the device.48  
  * Collecting extensive device fingerprinting data.49  
  * Transmitting data to third parties, including ByteDance's Volcengine cloud platform in China 47 and potentially undisclosed transmissions to state-linked entities like China Mobile.47  
  * Potential SQL injection vulnerabilities.47  
  * Employing anti-debugging techniques that hinder security analysis.47  
* **Malicious Attacks (January/February 2025):** Shortly after the high-profile launch of DeepSeek-R1, the company reported experiencing "large-scale malicious attacks" that disrupted new user registration.57 Reports also mentioned distributed denial-of-service (DDoS) attacks involving Mirai botnet variants.51

The pattern emerging from Deepseek's service offerings points towards significant and fundamental deficiencies in security and privacy practices. The exposed database and mobile app flaws suggest a lack of adherence to basic security hygiene, rather than exploitation of complex zero-day vulnerabilities. Coupled with data governance practices (storage in China, potential use for training) that are problematic for many international users and conflict with Western data protection regimes, utilizing Deepseek's direct services poses substantial risks. This is particularly true for enterprises handling sensitive data, intellectual property, or operating under strict regulatory oversight like GDPR or CCPA. The evidence suggests security and privacy may have been secondary considerations during Deepseek's rapid development, possibly reflecting a research-first orientation inherited from its quantitative finance background.9

### **B. Local Deployment Security Risks**

While deploying Deepseek's open-weight models locally mitigates the data privacy risks associated with their cloud services, it introduces a different set of security challenges related to the inherent properties and vulnerabilities of the models themselves, and the nature of open-weight software.

**Model Vulnerabilities (Jailbreaking, Tampering):**

* **Jailbreaking:** This refers to crafting specific prompts (adversarial inputs) designed to bypass a model's safety alignment training, causing it to generate harmful, restricted, or otherwise undesirable content.28 DeepSeek-R1, in particular, has demonstrated extreme susceptibility to jailbreaking in multiple independent tests:  
  * Cisco/Robust Intelligence reported a **100% attack success rate** against 50 harmful prompts from the HarmBench dataset, meaning the model failed to block any of them.28 This contrasts sharply with competitors like GPT-4o (86% blocked) and Gemini (64% blocked) in the same study.62  
  * AppSOC reported a 91% failure rate for jailbreaking tests.63  
  * Holistic AI found R1 produced safe responses to only 32% of jailbreaking prompts, compared to 100% for OpenAI's o1.64  
  * HiddenLayer and Trend Micro also confirmed vulnerabilities.34  
  * Examples of successful jailbreaks include generating functional malware (including ransomware code) and providing detailed instructions for illegal activities like money laundering.38  
  * The underlying cause is suspected to be related to Deepseek's cost-efficient training methods (RL, CoT self-evaluation, distillation) potentially compromising or omitting robust safety guardrails compared to competitors.28 The explicit Chain-of-Thought output in R1 might also expose internal workings, aiding attackers.34  
* **Tampering/Malicious Fine-tuning:** A significant risk with any open-weight model is that adversaries can download the model weights and modify them.68 Existing safety alignments (like refusal mechanisms) have been shown to be easily removable with relatively little fine-tuning effort.68 Attackers could potentially insert backdoors, remove safety controls entirely, or fine-tune the model for specific malicious purposes (e.g., generating highly targeted disinformation or phishing content). While research into "tamper-resistant" safeguards (like TAR 68) is ongoing, these are not standard features in current open-weight models, including Deepseek's.  
* **Arbitrary Code Execution Risk:** Some frameworks or methods for loading Deepseek models locally might require setting flags like trust\_remote\_code=True.23 This setting inherently trusts code bundled with the model weights and allows it to be executed, posing a direct security risk if the model source is compromised or contains malicious code.

**Data Poisoning and Malicious Fine-Tuning Threats:**

* **Data Poisoning:** This involves contaminating the data used for training or fine-tuning a model to introduce hidden vulnerabilities, biases, or malicious behaviors (e.g., "sleeper agent" behavior triggered by specific inputs).70 There are various ways this can occur, from an attacker deliberately fine-tuning a model with poisoned data, to unintentional biases creeping in through imperfect data curation, to malicious actors seeding web-scraped data with harmful content that later gets ingested during pre-training.72  
* **Relevance to Deepseek:** While the specifics of Deepseek's own pre-training data are not fully disclosed 66, raising the possibility of inherent biases or vulnerabilities 66, the open-weight nature creates a significant downstream risk. Malicious actors can take the released Deepseek models and fine-tune them locally using poisoned datasets, creating compromised versions that could be redistributed or used in attacks.70 Research also suggests that larger models may be more susceptible to learning harmful behaviors from smaller amounts of poisoned data, amplifying this risk as models scale.70

In essence, shifting to local deployment trades the service-related privacy risks of Deepseek's cloud offerings for model-centric security risks. Enterprises gain control over data flow but inherit the model's inherent vulnerabilities, particularly R1's demonstrated weakness against jailbreaking.28 The open-weight nature further exposes them to risks of tampering and malicious fine-tuning by third parties.68 Consequently, successful and secure local deployment of Deepseek models necessitates significant investment in enterprise-side security measures, including robust input validation, output filtering, continuous monitoring, anomaly detection, and potentially internal red-teaming efforts to proactively identify and mitigate behavioral risks. The convenience of open weights comes with the responsibility of securing the deployment.

## **VI. Feasibility Assessment: Local Training and Fine-Tuning**

While local inference offers privacy benefits, some organizations might consider local training or fine-tuning to further customize models or avoid any reliance on external pre-trained weights. However, assessing the feasibility requires understanding the immense resource requirements involved.

### **A. Resource Requirements for Enterprise Local Training/Fine-Tuning**

A crucial distinction must be made between *pre-training* a foundation model from scratch and *fine-tuning* an existing pre-trained model.

* **Pre-training:** Replicating the pre-training of models like DeepSeek-V3 or R1 locally is **highly impractical** for virtually all enterprises. Deepseek reported using a cluster of 2,048 Nvidia H800 GPUs running for approximately two months (totaling 2.788 million GPU hours) to pre-train V3 on 14.8 trillion tokens.8 Deepseek's parent company, High-Flyer, is known to possess substantial compute infrastructure, potentially including tens of thousands of high-end GPUs (A100s, H800s, H20s) acquired over years, representing hardware capital expenditures likely exceeding $1 billion.9 This scale of compute, data, and sustained operational effort is typically feasible only for specialized AI research labs or hyperscale cloud providers.  
* **Fine-tuning:** Fine-tuning an existing open-weight Deepseek model (like V3 Base or a distilled R1 model) on proprietary enterprise data is technically more feasible but remains a significant undertaking.94  
  * **GPU Requirements:** While less demanding than pre-training, fine-tuning still requires considerable GPU resources, often involving clusters of powerful GPUs. The exact needs depend on the base model size, the fine-tuning method (full parameter update vs. parameter-efficient fine-tuning like LoRA), and the dataset size. Examples suggest needing multiple high-VRAM GPUs (e.g., 64GB+ system RAM for fine-tuning mentioned generally 92, 8x 80GB GPUs cited for fine-tuning V2-Lite 24, A100s often needed 92). Accessing such GPU clusters involves either substantial capital investment or significant cloud rental costs.95  
  * **Data Requirements:** Effective fine-tuning requires high-quality, domain-specific datasets. Preparing this data – collecting, cleaning, labeling, and formatting – can be a major bottleneck, requiring significant time, effort, and domain expertise.94 While techniques like synthetic data generation might reduce this burden 94, curating a suitable dataset remains a critical challenge.  
  * **Expertise:** Successfully fine-tuning large models demands specialized skills in machine learning engineering and data science. This includes setting up the training pipeline, managing distributed training jobs, tuning hyperparameters, evaluating model performance, and ensuring alignment and safety.94

### **B. Cost-Benefit Analysis vs. Deepseek's Training Scale**

Evaluating the cost-benefit of local fine-tuning involves comparing the substantial costs against the potential benefits of a customized model.

* **Costs:**  
  * *Hardware/Compute:* Acquiring or renting the necessary GPU clusters represents a major expense. High-end GPUs like Nvidia H100s or A100s can cost upwards of $1.30 \- $2.79+ per hour to rent on specialized cloud platforms.95 Sustained fine-tuning runs can quickly accumulate significant compute costs.  
  * *Data Preparation:* The effort involved in creating a suitable fine-tuning dataset can translate to high internal personnel costs or external vendor fees.94  
  * *Personnel:* Salaries for the skilled ML engineers and data scientists required to manage the process are considerable.78  
  * *Infrastructure:* Energy, cooling, and maintenance for local clusters add to the total cost of ownership.78  
* **Benefits:**  
  * *Customization:* Tailoring the model to specific enterprise terminology, data, workflows, or tasks can potentially yield higher performance and relevance than general-purpose models.94  
  * *Data Privacy:* The fine-tuning process itself can be conducted entirely on-premises, ensuring that proprietary training data remains confidential.  
* **Comparison:** Deepseek's *claimed* pre-training compute cost of \~$5.6M for V3 7, while debated, sets a benchmark for the *potential* efficiency achievable with their architecture and methods at scale. However, this figure is for pre-training by Deepseek, not fine-tuning by an enterprise. The cost for an enterprise to fine-tune locally will depend heavily on the scale of the task, but will inevitably involve significant expenditure on compute, data, and personnel.94

Given the resource intensity, full local pre-training of Deepseek-scale models is clearly out of reach for typical enterprises. Local fine-tuning, while technically enabled by the availability of open-weight models, remains a complex and costly proposition. It requires substantial investment in specialized hardware (GPU clusters), significant effort in data curation, and dedicated ML expertise. This approach is likely only justifiable for organizations with very specific, high-value use cases where generic models are inadequate, API-based fine-tuning (if offered) poses unacceptable privacy risks, and the necessary resources (budget, talent, data) are readily available. For most enterprises, leveraging the pre-trained open-weight models via local inference (Section IV) or potentially using API-based services (with careful risk assessment, Section V.A) will be far more practical and cost-effective than attempting substantial local fine-tuning. Emerging techniques like context stacking 96 might lower the barrier for certain types of customization in the future, but traditional fine-tuning remains a resource-heavy endeavor.

## **VII. Cost Considerations: Training Efficiency vs. Operational Expenditure**

A central element of Deepseek's narrative revolves around cost-efficiency. However, it is crucial for enterprises to distinguish between Deepseek's *training* costs and their own potential *operational* costs when deploying these models.

### **A. Analyzing Deepseek's Claimed Training Cost Advantages**

Deepseek has garnered significant attention for its claims of training powerful LLMs at a fraction of the cost incurred by competitors.1 The most prominent example is the reported compute cost of approximately $5.6 million for pre-training DeepSeek-V3 7, a figure dramatically lower than the hundreds of millions often estimated for models like GPT-4.4

Several factors likely contribute to Deepseek's computational efficiency during training:

* **Architectural Innovations:** The use of Mixture-of-Experts (MoE) architecture inherently reduces the FLOPs required per token by activating only a subset of parameters.3 Multi-head Latent Attention (MLA) might also offer training benefits by reducing activation memory.22  
* **Optimized Training Frameworks:** Deepseek pioneered FP8 mixed-precision training for V3, which can reduce memory usage and potentially accelerate computation on compatible hardware.20 They also report co-designing algorithms, frameworks, and hardware to overcome communication bottlenecks in MoE training.20  
* **Hardware Utilization:** While potentially using less advanced chips like H800s due to export controls 8, optimizing their use within large, dedicated clusters (built by parent High-Flyer) could yield high efficiency.9  
* **Data Efficiency:** Techniques like multi-token prediction 20 and potentially efficient data curation or synthetic data generation 94 might improve sample efficiency, requiring fewer tokens (and thus compute) to reach a given performance level.

However, the $5.6 million figure is subject to considerable debate and likely represents only a fraction of the total cost.78 Critics argue it excludes critical expenditures such as:

* Research and Development (R\&D) costs, including failed experiments and development of precursor models.78  
* Hardware acquisition and Total Cost of Ownership (TCO), which for Deepseek's large clusters could be in the hundreds of millions or even billions of dollars.9  
* Data acquisition, cleaning, and preparation costs.78  
* Personnel costs for large research and engineering teams.78  
* Infrastructure costs like electricity, cooling, and maintenance.78

Industry analysts estimate Deepseek's total investment is likely far higher than the reported training compute cost.9 Nevertheless, even if the absolute dollar figure is debatable, the underlying evidence points towards genuine advancements in *computational efficiency* (e.g., GPU hours per trillion tokens processed 20) achieved through their specific architectural and training methodologies.3

### **B. Hardware Costs for Local Inference Operations**

While Deepseek may have achieved efficiencies in their own training process, enterprises choosing to deploy these models locally must consider their *own* operational expenditures, primarily driven by hardware costs for inference.

As detailed in Section IV.B, running Deepseek models locally requires hardware commensurate with the model size and desired performance.

* **Distilled Models (e.g., 7B-70B):** These can often be run on high-end consumer GPUs (like RTX 30/40 series) or prosumer/workstation cards, requiring investments ranging from hundreds to several thousand dollars per machine.83  
* **Full Models (671B):** Running these locally necessitates significant investment in server-grade hardware, either high-RAM CPU systems (512GB-1TB+ RAM) or large multi-GPU clusters (e.g., multiple A100s/H100s), potentially costing tens or hundreds of thousands of dollars per system.83

Therefore, the cost for an enterprise to *run* Deepseek models locally can be substantial, potentially negating the narrative of "low cost" derived from Deepseek's *training* efficiency claims, unless opting for smaller distilled models or possessing suitable existing infrastructure.

API Costs as an Alternative:  
For organizations unable or unwilling to invest in local hardware, Deepseek offers API access, which presents a different cost structure based on token usage.

* **Models:** Two main endpoints are typically offered: deepseek-chat (powered by V3, optimized for conversation) and deepseek-reasoner (powered by R1, optimized for complex tasks).159  
* **Pricing:** Costs are measured per million tokens processed, with different rates for input and output tokens. A unique feature is differential pricing based on cache hits: inputs that match previously processed prefixes are charged at a significantly lower "cache hit" rate.159 There are also standard and discounted (off-peak UTC times) rates.160  
  * *Deepseek-Chat (Standard Rates):* \~$0.07/M input (cache hit), \~$0.27/M input (cache miss), \~$1.10/M output.159  
  * *Deepseek-Reasoner (Standard Rates):* \~$0.14/M input (cache hit), \~$0.55/M input (cache miss), \~$2.19/M output.159  
* **Comparison:** These API prices are significantly lower than those of major Western competitors like OpenAI (GPT-4o) or Anthropic (Claude 3.5 Sonnet).18 For example, GPT-4 API costs can be tens of dollars per million tokens.161  
* **Recent Changes:** Deepseek reportedly ended promotional pricing in early February 2025 due to high demand, increasing prices from initial launch levels, though they remain comparatively low.162

**Cost Comparison: Local Inference vs. API Usage (Illustrative)**

| Scenario | Deepseek Model | Deployment | Estimated Cost Driver | Relative Cost Level | Privacy/Security Risk |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Moderate Use, High Privacy | R1 Distill 7B/8B | Local | Hardware (e.g., 1x RTX 3080/4070) \+ Energy/Maintenance | Medium (CapEx) | Low (Privacy), Medium (Model Security) |
| High Use, High Privacy | R1 Distill 32B/70B | Local | Hardware (e.g., 1-2x RTX 4090/A100) \+ Energy/Maintenance | High (CapEx) | Low (Privacy), Medium (Model Security) |
| Max Capability, High Privacy | V3/R1 Full (671B) | Local | Hardware (Multi-GPU Cluster/High-RAM Server) \+ Energy/Maint | Very High (CapEx) | Low (Privacy), Medium (Model Security) |
| Moderate Use, Lower Privacy | deepseek-chat | API | Per-token fees (Input/Output, Cache Hit/Miss) | Low (OpEx) | High (Privacy/Service Security) |
| Complex Tasks, Lower Privacy | deepseek-reasoner | API | Per-token fees (Higher than chat) | Low-Medium (OpEx) | High (Privacy/Service Security) |

*Note: Relative Cost Levels are illustrative. Actual costs depend heavily on usage volume, hardware prices, energy costs, and API pricing fluctuations.*

This analysis reveals a complex trade-off for enterprises. Deepseek's API offers a very low *operational* cost but entails significant privacy and security risks due to data handling and service vulnerabilities. Local deployment provides strong data *privacy* but requires substantial *capital* expenditure on hardware, especially for larger models, and still carries *model security* risks (like jailbreaking). The "low-cost AI" narrative associated with Deepseek primarily reflects potential training efficiencies and competitive API pricing, not necessarily low TCO for enterprises pursuing secure, private local deployment of its most capable models.

## **VIII. Geopolitical Landscape and Recent Developments (as of April 2025\)**

The emergence and rapid advancement of Deepseek AI cannot be fully understood without considering the broader geopolitical context, particularly the intensifying technological competition between the United States and China, and the resulting policy actions and security concerns.

### **A. Findings from the U.S. House Select Committee Report (April 16, 2025\)**

In April 2025, the U.S. House Select Committee on the Chinese Communist Party (CCP) released a significant bipartisan report titled "DeepSeek Unmasked: Exposing the CCP's Latest Tool For Spying, Stealing, and Subverting U.S. Export Control Restrictions".99 This report presented a highly critical assessment of Deepseek, labeling it a "profound threat" to U.S. national security.1

The Committee's key allegations included:

* **Data Espionage:** Accusations that Deepseek's application funnels sensitive data from American users back to China.1 The report specifically cited Deepseek's privacy policy acknowledging data storage in China and highlighted technical findings suggesting backend infrastructure connections to China Mobile, a state-owned telecom designated by the U.S. government as a Chinese military company.62 It also noted integrations with tracking tools from other Chinese tech giants like ByteDance, Baidu, and Tencent.99  
* **Information Manipulation/Censorship:** Claims that the Deepseek chatbot covertly manipulates or suppresses responses related to topics politically sensitive to the CCP (such as democracy, Taiwan, Hong Kong, human rights abuses) in alignment with Chinese law and propaganda directives, without disclosing this filtering to users.73 Testing reportedly showed this occurred in 85% of relevant cases.99  
* **Intellectual Property Theft:** Assertions that it is "highly likely" Deepseek used unlawful "model distillation" techniques, effectively stealing capabilities from leading U.S. AI models by circumventing their safeguards and copying their outputs.97 The report cited testimony from OpenAI and alleged Deepseek personnel used aliases and international banking channels to access U.S. models.99  
* **Export Control Circumvention:** Allegations that Deepseek's models appear to be powered by tens of thousands of advanced Nvidia chips (including potentially restricted H800s or others) that may have been obtained illicitly, circumventing U.S. export controls.9  
* **CCP Links:** The report highlighted founder Liang Wenfeng's alleged close ties to the CCP and ideological alignment with "Xi Jinping Thought".97

Concurrent with the report's release, the Committee sent a formal letter to Nvidia demanding detailed information about its chip sales in China and Southeast Asia (specifically Singapore, Malaysia, and 9 other nations) dating back to 2020, seeking to determine how its technology might have powered Deepseek despite restrictions.100 Nvidia responded publicly, stating it complies fully with government export instructions and that reported revenue in locations like Singapore often reflects the billing addresses of subsidiaries of U.S. customers.101

### **B. Potential U.S. Government Actions and Export Controls**

The House Committee report explicitly recommended swift action from the U.S. government, including expanding export controls, improving enforcement, addressing risks specifically from PRC AI models, and preparing for strategic surprises in AI development.97

This aligns with broader trends and specific actions taken or considered around the time:

* **Potential Penalties/Bans:** Reports emerged suggesting the Trump administration was considering penalties to block Deepseek from acquiring U.S. technology and potentially restricting American access to Deepseek's services.104  
* **Existing/Tightened Chip Controls:** The U.S. government had already implemented controls restricting the export of high-performance AI chips (like Nvidia's H100 and later the H800/A800) to China.2 In April 2025, the administration placed restrictions on the export of Nvidia's H20 chip, which was specifically designed as a lower-performance alternative for the Chinese market after earlier controls.97 These actions directly impact the hardware available to companies like Deepseek.113 The Department of Commerce also introduced controls on AI model weights themselves in January 2025 (ECCN 4E091).113  
* **Government Device Bans:** Prior to and following the House report, there was a push to ban Deepseek from government devices. Representatives Gottheimer and LaHood introduced the "No DeepSeek on Government Devices Act" (H.R. 1121\) in February 2025 and urged state governors to implement bans.108 Several state Attorneys General supported this federal bill.109 States like South Dakota and Oklahoma implemented their own bans on state devices 109, and federal agencies including the Pentagon, Navy, and NASA reportedly blocked or advised against its use due to security concerns.50  
* **AI Standards Development:** The National Institute of Standards and Technology (NIST) continued its work on AI standards, releasing a final report on Adversarial Machine Learning taxonomy and terminology in March 2025, providing guidance relevant to securing AI systems against manipulation.109

### **C. Broader Implications in US-China AI Competition**

Deepseek's rapid rise and the subsequent U.S. response are deeply intertwined with the broader strategic competition in AI between the two nations.1

* **Export Control Effectiveness Debate:** Deepseek's success, particularly its ability to train high-performing models allegedly using less advanced or restricted chips, fueled debate about the efficacy of U.S. export controls aimed at slowing China's AI progress.2 Some argue it demonstrates the controls are futile or even counterproductive, potentially spurring Chinese innovation in efficiency.5 Others contend it highlights existing loopholes or the need for stricter enforcement and adaptation of controls.9 The timing of Deepseek's major releases was even interpreted by some analysts as a strategic move by China to influence the incoming Trump administration against continuing stringent controls.2  
* **Open-Source vs. Closed Models:** Deepseek's embrace of open-weight models 74 contrasts with the more closed approach of some leading U.S. labs. In the geopolitical context, China may be strategically promoting open-source models from domestic companies like Deepseek, while some U.S. policymakers advocate for more centralized, closed paths, citing misuse risks.111 This raises questions about the strategic implications of open-source AI in an era of tech competition and the dual-use dilemma inherent in releasing powerful, adaptable models.68  
* **Model Alignment and Influence:** Concerns extend beyond technical capabilities to the potential for models like Deepseek to reflect CCP ideology or censorship directives.34 The prospect of AI systems subtly shaping user perspectives or exhibiting biased recommendations (e.g., hawkish foreign policy suggestions observed in testing 111) adds another dimension to the national security concerns.

Deepseek has thus become a potent symbol in the US-China AI narrative. It represents China's undeniable progress and capacity for innovation in AI, even under external pressure. Simultaneously, it embodies the security, privacy, and espionage concerns that fuel U.S. anxieties about Chinese technology. The strong reaction from the U.S. government, particularly the House Select Committee report 99, indicates that Deepseek is viewed not merely as a commercial competitor but as an entity intertwined with state interests and posing strategic risks. This geopolitical dimension significantly elevates the risk profile for any enterprise considering adopting Deepseek technology, necessitating careful consideration beyond purely technical or economic factors. Further U.S. policy actions targeting Deepseek or related technologies appear likely.

## **IX. Conclusion and Strategic Recommendations**

Deepseek AI presents a complex and multifaceted proposition for enterprises. Its rapid development has yielded large language models demonstrating impressive performance on par with or exceeding established Western benchmarks, particularly in reasoning (DeepSeek-R1) and overall capabilities combined with claimed cost-efficiency (DeepSeek-V3).13 Innovations in MoE and MLA architectures contribute to this efficiency 19, and the company's commitment to releasing open-weight models offers significant accessibility.2 However, these potential advantages are counterbalanced by substantial and well-documented risks. Deepseek's API, chat services, and mobile applications suffer from alarming security vulnerabilities, including unencrypted data transmission, weak cryptography, and a major database exposure incident.43 Furthermore, its data privacy practices, involving the storage of user data in China and potential use for model training without clear opt-outs, raise serious compliance issues (GDPR, CCPA) and national security concerns.33 The models themselves, especially R1, exhibit extreme vulnerability to jailbreaking 28, and the open-weight nature introduces risks of tampering and malicious fine-tuning.68 Additionally, the company faces intense geopolitical scrutiny, highlighted by the U.S. House Select Committee report labeling it a national security threat.99

Enterprises considering Deepseek technology must navigate a challenging balancing act. The allure of potentially high-performing, cost-effective, and customizable open-weight models must be weighed against the significant security, privacy, compliance, and geopolitical risks. Adoption decisions require a rigorous, context-specific risk assessment, considering the sensitivity of the data involved, the applicable regulatory landscape, and the organization's risk tolerance.

Based on the analysis presented in this report, the following strategic recommendations are offered:

1. **Avoid Direct API/Chat/App Usage for Sensitive Data:** Enterprises should **strongly avoid** using Deepseek's public API, web chat interface, or mobile applications for any workflows involving sensitive, confidential, proprietary, or regulated data. The documented security flaws 43, combined with the policy of storing data in China 41 and potential use for model training 41, create an unacceptably high risk profile for such use cases, particularly for organizations subject to GDPR, CCPA, or other stringent data protection laws.  
2. **Prioritize Local Deployment for Privacy:** For enterprises wishing to leverage Deepseek's model capabilities while maintaining data control, **local, on-premises deployment of the open-weight models is the only recommended approach**.31 This strategy keeps all inference data within the enterprise's environment, mitigating the primary privacy and data residency risks associated with Deepseek's services.  
3. **Acknowledge Local Deployment Costs:** While providing privacy benefits, local deployment requires substantial hardware investment, especially for the larger, more capable models (e.g., the full 671B V3/R1).83 Enterprises should carefully evaluate the TCO, considering GPU/server costs, energy, and maintenance. The **distilled R1 models (1.5B-70B)** offer a more feasible entry point for organizations with moderate hardware budgets.83 Deepseek's claimed *training* efficiency does not equate to low *operational* cost for local deployment.  
4. **Implement Robust Local Security Guardrails:** Critically, local deployment **does not** eliminate risks inherent in the models themselves. Given the demonstrated vulnerabilities, particularly DeepSeek-R1's susceptibility to jailbreaking 28, enterprises deploying these models locally **must implement their own comprehensive security controls**. This includes, but is not limited to:  
   * Rigorous input validation and sanitization.  
   * Strict output filtering to detect and block harmful, biased, or inappropriate content.  
   * Monitoring model behavior for anomalies or signs of compromise.  
   * Implementing strong access controls around the model deployment.  
   * Considering internal red-teaming exercises to proactively identify vulnerabilities in the deployed context.  
   * Avoiding configurations that require trust\_remote\_code=True unless the code's origin and safety can be thoroughly verified.34  
5. **Approach Local Training/Fine-tuning with Extreme Caution:** Full pre-training is infeasible. Local fine-tuning, while technically possible, is a complex, resource-intensive process requiring significant investment in GPU clusters, specialized data curation, and expert personnel. It should only be considered for highly specific, high-value use cases where the benefits clearly outweigh the substantial costs and effort, and where API-based fine-tuning is not an option due to privacy constraints.  
6. **Monitor Geopolitical and Security Developments:** The situation surrounding Deepseek is dynamic. Enterprises should continuously monitor for updates regarding U.S. government actions (sanctions, export controls), further independent security audits or vulnerability disclosures, and Deepseek's own responses to security concerns (e.g., patches for mobile apps, improvements to model safety). Geopolitical risk must be factored into any long-term strategic reliance on Deepseek technology.

In conclusion, Deepseek AI embodies both the remarkable pace of progress in artificial intelligence and the significant risks that accompany powerful, rapidly deployed technologies, especially those originating from complex geopolitical contexts. While its models offer potential advantages in performance and accessibility via open weights, the associated security and privacy concerns, particularly with its direct services, are substantial. Responsible enterprise adoption necessitates a clear-eyed assessment of these risks and a commitment to implementing rigorous technical controls and governance frameworks, strongly favoring carefully secured local deployments over reliance on the company's cloud infrastructure for any sensitive operations.

#### **Works cited**

1. DeepSeek accused of risking US security with data to China \- Tech in Asia, accessed April 17, 2025, [https://www.techinasia.com/news/deepseek-accused-risking-security-data-china](https://www.techinasia.com/news/deepseek-accused-risking-security-data-china)  
2. What is DeepSeek, the Chinese AI company upending the stock market? \- AP News, accessed April 17, 2025, [https://apnews.com/article/deepseek-ai-china-f4908eaca221d601e31e7e3368778030](https://apnews.com/article/deepseek-ai-china-f4908eaca221d601e31e7e3368778030)  
3. DeepSeek: Rewriting the Rules of AI Development | CSA \- Cloud Security Alliance, accessed April 17, 2025, [https://cloudsecurityalliance.org/blog/2025/01/29/deepseek-rewriting-the-rules-of-ai-development](https://cloudsecurityalliance.org/blog/2025/01/29/deepseek-rewriting-the-rules-of-ai-development)  
4. What is DeepSeek? Here's a quick guide to the Chinese AI company | PBS News, accessed April 17, 2025, [https://www.pbs.org/newshour/science/what-is-deepseek-heres-a-quick-guide-to-the-chinese-ai-company](https://www.pbs.org/newshour/science/what-is-deepseek-heres-a-quick-guide-to-the-chinese-ai-company)  
5. What's DeepSeek, China's AI startup sending shockwaves through global tech? \- Al Jazeera, accessed April 17, 2025, [https://www.aljazeera.com/economy/2025/1/28/why-chinas-ai-startup-deepseek-is-sending-shockwaves-through-global-tech](https://www.aljazeera.com/economy/2025/1/28/why-chinas-ai-startup-deepseek-is-sending-shockwaves-through-global-tech)  
6. DeepSeek \- Wikipedia, accessed April 17, 2025, [https://en.wikipedia.org/wiki/DeepSeek](https://en.wikipedia.org/wiki/DeepSeek)  
7. Why DeepSeek is a Game-Changer in Artificial Intelligence, accessed April 17, 2025, [https://www.pageon.ai/blog/deepseek](https://www.pageon.ai/blog/deepseek)  
8. Taking Stock of the DeepSeek Shock | FSI \- Stanford Cyber Policy Center, accessed April 17, 2025, [https://cyber.fsi.stanford.edu/publication/taking-stock-deepseek-shock](https://cyber.fsi.stanford.edu/publication/taking-stock-deepseek-shock)  
9. DeepSeek: A Deep Dive \- CSIS, accessed April 17, 2025, [https://www.csis.org/analysis/deepseek-deep-dive](https://www.csis.org/analysis/deepseek-deep-dive)  
10. Who Owns Deepseek? \- Business Model Analyst, accessed April 17, 2025, [https://businessmodelanalyst.com/who-owns-deepseek/](https://businessmodelanalyst.com/who-owns-deepseek/)  
11. www.techtarget.com, accessed April 17, 2025, [https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know\#:\~:text=DeepSeek's%20aim%20is%20to%20achieve,Reinforcement%20learning.](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=DeepSeek's%20aim%20is%20to%20achieve,Reinforcement%20learning.)  
12. DeepSeek AI | Latest News & Updates \- Apr 14, 2025 Release \- RivalSense, accessed April 17, 2025, [https://rivalsense.co/intel/deepseek-ai-latest-news-updates-apr-14-2025-release/](https://rivalsense.co/intel/deepseek-ai-latest-news-updates-apr-14-2025-release/)  
13. DeepSeek Open-Sources DeepSeek-R1 LLM with Performance Comparable to OpenAI's o1 Model \- InfoQ, accessed April 17, 2025, [https://www.infoq.com/news/2025/02/deepseek-r1-release/](https://www.infoq.com/news/2025/02/deepseek-r1-release/)  
14. DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1 \- PromptHub, accessed April 17, 2025, [https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1](https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1)  
15. Notes on Deepseek r1: Just how good it is compared to OpenAI o1 : r/LocalLLaMA \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes\_on\_deepseek\_r1\_just\_how\_good\_it\_is\_compared/](https://www.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes_on_deepseek_r1_just_how_good_it_is_compared/)  
16. deepseek-r1 Model by Deepseek-ai \- NVIDIA NIM APIs, accessed April 17, 2025, [https://build.nvidia.com/deepseek-ai/deepseek-r1/modelcard](https://build.nvidia.com/deepseek-ai/deepseek-r1/modelcard)  
17. LLM Leaderboard DeepSeek: Performance Insights \- BytePlus, accessed April 17, 2025, [https://www.byteplus.com/en/topic/516139](https://www.byteplus.com/en/topic/516139)  
18. DeepSeek performance: How it compares to top AI models \- Bracai, accessed April 17, 2025, [https://www.bracai.eu/post/deepseek-performance](https://www.bracai.eu/post/deepseek-performance)  
19. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model, accessed April 17, 2025, [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)  
20. \[2412.19437\] DeepSeek-V3 Technical Report \- arXiv, accessed April 17, 2025, [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437)  
21. Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2502.14837v1](https://arxiv.org/html/2502.14837v1)  
22. arxiv.org, accessed April 17, 2025, [https://arxiv.org/pdf/2412.19437](https://arxiv.org/pdf/2412.19437)  
23. deepseek-ai/DeepSeek-V3 · Hugging Face, accessed April 17, 2025, [https://huggingface.co/deepseek-ai/DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)  
24. deepseek-ai/DeepSeek-V2-Lite \- Hugging Face, accessed April 17, 2025, [https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite)  
25. arxiv.org, accessed April 17, 2025, [https://arxiv.org/pdf/2405.04434](https://arxiv.org/pdf/2405.04434)  
26. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, accessed April 17, 2025, [https://huggingface.co/papers/2501.12948](https://huggingface.co/papers/2501.12948)  
27. arxiv.org, accessed April 17, 2025, [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)  
28. Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models \- Cisco Blogs, accessed April 17, 2025, [https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models](https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models)  
29. How was DeepSeek-R1 built; For dummies : r/LLMDevs \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LLMDevs/comments/1ibhpqw/how\_was\_deepseekr1\_built\_for\_dummies/](https://www.reddit.com/r/LLMDevs/comments/1ibhpqw/how_was_deepseekr1_built_for_dummies/)  
30. arxiv.org, accessed April 17, 2025, [https://arxiv.org/pdf/2501.12948](https://arxiv.org/pdf/2501.12948)  
31. Real-Time AI Pipeline with DeepSeek, Ollama and Pathway, accessed April 17, 2025, [https://pathway.com/blog/deepseek-ollama/](https://pathway.com/blog/deepseek-ollama/)  
32. Got DeepSeek R1 running locally \- Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??) : r/LocalLLaMA \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1i6gahy/got\_deepseek\_r1\_running\_locally\_full\_setup\_guide/](https://www.reddit.com/r/LocalLLaMA/comments/1i6gahy/got_deepseek_r1_running_locally_full_setup_guide/)  
33. A deep(er) dive into DeepSeek's privacy policies \- Digiday, accessed April 17, 2025, [https://digiday.com/media/a-deeper-dive-into-deepseeks-privacy-policies/](https://digiday.com/media/a-deeper-dive-into-deepseeks-privacy-policies/)  
34. DeepSh\*t: Exposing the Security Risks of DeepSeek-R1 \- HiddenLayer, accessed April 17, 2025, [https://hiddenlayer.com/innovation-hub/deepsht-exposing-the-security-risks-of-deepseek-r1/](https://hiddenlayer.com/innovation-hub/deepsht-exposing-the-security-risks-of-deepseek-r1/)  
35. 6 Ways to Stay Private and Secure on DeepSeek \- Anonyome Labs, accessed April 17, 2025, [https://anonyome.com/resources/blog/6-ways-to-stay-private-and-secure-on-deepseek/](https://anonyome.com/resources/blog/6-ways-to-stay-private-and-secure-on-deepseek/)  
36. Aethir Checker Nodes Launch: Start Earning Rewards, accessed April 17, 2025, [https://www.aethir.com/blog-posts/aethir-checker-nodes-launch-start-earning-rewards](https://www.aethir.com/blog-posts/aethir-checker-nodes-launch-start-earning-rewards)  
37. DeepSeek: Security and Data Privacy Concerns, accessed April 17, 2025, [https://www.harmonic.security/blog-posts/deepseek-security-and-data-privacy-concerns](https://www.harmonic.security/blog-posts/deepseek-security-and-data-privacy-concerns)  
38. DeepSeek Security, Privacy, and Governance: Hidden Risks in Open-Source AI \- Theori, accessed April 17, 2025, [https://theori.io/blog/deepseek-security-privacy-and-governance-hidden-risks-in-open-source-ai](https://theori.io/blog/deepseek-security-privacy-and-governance-hidden-risks-in-open-source-ai)  
39. Privacy Concerns with LLM Models (and DeepSeek in particular) : r/LocalLLaMA \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1i1ugj5/privacy\_concerns\_with\_llm\_models\_and\_deepseek\_in/](https://www.reddit.com/r/LocalLLaMA/comments/1i1ugj5/privacy_concerns_with_llm_models_and_deepseek_in/)  
40. DeepSeek: Legal Considerations for Enterprise Users | Insights | Ropes & Gray LLP, accessed April 17, 2025, [https://www.ropesgray.com/en/insights/alerts/2025/01/deepseek-legal-considerations-for-enterprise-users](https://www.ropesgray.com/en/insights/alerts/2025/01/deepseek-legal-considerations-for-enterprise-users)  
41. DeepSeek Privacy Policy, accessed April 17, 2025, [https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy.html](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy.html)  
42. DeepSeek collects keystroke data and more, storing it in Chinese servers \- Mashable, accessed April 17, 2025, [https://mashable.com/article/deepseek-ai-privacy-policy-keystroke-data-chinese-servers](https://mashable.com/article/deepseek-ai-privacy-policy-keystroke-data-chinese-servers)  
43. Wiz Research Uncovers Exposed DeepSeek Database Leaking Sensitive Information, Including Chat History, accessed April 17, 2025, [https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak](https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak)  
44. DeepSeek is Wide Open for Abuse. Here's Why That's a Problem. \- Ironscales, accessed April 17, 2025, [https://ironscales.com/blog/deepseek-is-wide-open-for-abuse-heres-why-thats-a-problem](https://ironscales.com/blog/deepseek-is-wide-open-for-abuse-heres-why-thats-a-problem)  
45. Copilot's Weakness, DeepSeek Data exposed, Backdoor in Contec CMS8000 & Apple's Zero-Day | DevCentral, accessed April 17, 2025, [https://community.f5.com/kb/security-insights/copilot%E2%80%99s-weakness-deepseek-data-exposed-backdoor-in-contec-cms8000--apples-zero/339518](https://community.f5.com/kb/security-insights/copilot%E2%80%99s-weakness-deepseek-data-exposed-backdoor-in-contec-cms8000--apples-zero/339518)  
46. DeepSeek database left open, exposing sensitive info \- The Register, accessed April 17, 2025, [https://www.theregister.com/2025/01/30/deepseek\_database\_left\_open/](https://www.theregister.com/2025/01/30/deepseek_database_left_open/)  
47. A Deep Peek at DeepSeek \- SecurityScorecard, accessed April 17, 2025, [https://securityscorecard.com/blog/a-deep-peek-at-deepseek/](https://securityscorecard.com/blog/a-deep-peek-at-deepseek/)  
48. Forensic Analysis and Security Implications of DeepSeek – Blog | DigForCE Lab, accessed April 17, 2025, [https://blogs.dsu.edu/digforce/2025/04/09/forensic-analysis-and-security-implications-of-deepseek/](https://blogs.dsu.edu/digforce/2025/04/09/forensic-analysis-and-security-implications-of-deepseek/)  
49. NowSecure Uncovers Multiple Security and Privacy Flaws in DeepSeek iOS Mobile App, accessed April 17, 2025, [https://www.nowsecure.com/blog/2025/02/06/nowsecure-uncovers-multiple-security-and-privacy-flaws-in-deepseek-ios-mobile-app/](https://www.nowsecure.com/blog/2025/02/06/nowsecure-uncovers-multiple-security-and-privacy-flaws-in-deepseek-ios-mobile-app/)  
50. Experts Flag Security, Privacy Risks in DeepSeek AI App, accessed April 17, 2025, [https://krebsonsecurity.com/2025/02/experts-flag-security-privacy-risks-in-deepseek-ai-app/](https://krebsonsecurity.com/2025/02/experts-flag-security-privacy-risks-in-deepseek-ai-app/)  
51. Threat Intelligence \- DeepSeek App Exposes Sensitive User and Device Data Due to Lack of Encryption \- Quorum Cyber, accessed April 17, 2025, [https://www.quorumcyber.com/threat-intelligence/deepseek-app-exposes-sensitive-user-and-device-data-due-to-lack-of-encryption/](https://www.quorumcyber.com/threat-intelligence/deepseek-app-exposes-sensitive-user-and-device-data-due-to-lack-of-encryption/)  
52. DeepSeek App's Security Failures: How Approov Could Have Prevented the Damage, accessed April 17, 2025, [https://approov.io/blog/deepseek-apps-security-failures-how-approov-could-have-prevented-the-damage](https://approov.io/blog/deepseek-apps-security-failures-how-approov-could-have-prevented-the-damage)  
53. DeepSeek app security and privacy weaknesses | Information Systems & Technology | University of Waterloo, accessed April 17, 2025, [https://uwaterloo.ca/information-systems-technology/news/deepseek-app-security-and-privacy-weaknesses](https://uwaterloo.ca/information-systems-technology/news/deepseek-app-security-and-privacy-weaknesses)  
54. Safety Evaluation of DeepSeek Models in Chinese Contexts \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2502.11137v1](https://arxiv.org/html/2502.11137v1)  
55. Cisco Uncovers Critical Security Flaws in DeepSeek R1 AI Model \- theCUBE Research, accessed April 17, 2025, [https://thecuberesearch.com/cisco-uncovers-critical-security-flaws-in-deepseek-r1-ai-model/](https://thecuberesearch.com/cisco-uncovers-critical-security-flaws-in-deepseek-r1-ai-model/)  
56. The Dangers of Rushing into AI Adoption: Lessons from DeepSeek | A10 Networks, accessed April 17, 2025, [https://www.a10networks.com/blog/the-dangers-of-rushing-into-ai-adoption-lessons-from-deepseek/](https://www.a10networks.com/blog/the-dangers-of-rushing-into-ai-adoption-lessons-from-deepseek/)  
57. Cyber security implications of DeepSeek's open-source AI model, accessed April 17, 2025, [https://www.cshub.com/threat-defense/articles/cyber-security-implications-deepseek-ai](https://www.cshub.com/threat-defense/articles/cyber-security-implications-deepseek-ai)  
58. About Deep Seek AI | Leading AI Innovation & Research, accessed April 17, 2025, [https://deepseek.ai/about](https://deepseek.ai/about)  
59. DeepSeek V3 LLM NVIDIA H200 GPU Inference Benchmarking — Blog \- DataCrunch, accessed April 17, 2025, [https://datacrunch.io/blog/deepseek-v3-llm-nvidia-h200-gpu-inference-benchmarking](https://datacrunch.io/blog/deepseek-v3-llm-nvidia-h200-gpu-inference-benchmarking)  
60. DeepSeek-R1 Paper Explained \- A New RL LLMs Era in AI? \- YouTube, accessed April 17, 2025, [https://www.youtube.com/watch?v=DCqqCLlsIBU\&pp=0gcJCfcAhR29\_xXO](https://www.youtube.com/watch?v=DCqqCLlsIBU&pp=0gcJCfcAhR29_xXO)  
61. openEuler × DeepSeek 2: vLLM Deployment Guide (CPU \+ GPU), accessed April 17, 2025, [https://www.openeuler.org/en/blog/03-DeepSeek2/2.html](https://www.openeuler.org/en/blog/03-DeepSeek2/2.html)  
62. Delving into the Dangers of DeepSeek \- CSIS, accessed April 17, 2025, [https://www.csis.org/analysis/delving-dangers-deepseek](https://www.csis.org/analysis/delving-dangers-deepseek)  
63. Testing the DeepSeek-R1 Model: A Pandora's Box of Security Risks \- AppSOC Blog, accessed April 17, 2025, [https://www.appsoc.com/blog/testing-the-deepseek-r1-model-a-pandoras-box-of-security-risks](https://www.appsoc.com/blog/testing-the-deepseek-r1-model-a-pandoras-box-of-security-risks)  
64. DeepSeek R1 Red Teaming & Jailbreaking Audit \- Holistic AI, accessed April 17, 2025, [https://www.holisticai.com/red-teaming/deepseek-r1](https://www.holisticai.com/red-teaming/deepseek-r1)  
65. Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security | Trend Micro (US), accessed April 17, 2025, [https://www.trendmicro.com/en\_us/research/25/c/exploiting-deepseek-r1.html](https://www.trendmicro.com/en_us/research/25/c/exploiting-deepseek-r1.html)  
66. The Hidden Risks Of Open Source AI: Why DeepSeek-R1's Transparency Isn't Enough, accessed April 17, 2025, [https://www.forbes.com/councils/forbestechcouncil/2025/03/06/the-hidden-risks-of-open-source-ai-why-deepseek-r1s-transparency-isnt-enough/](https://www.forbes.com/councils/forbestechcouncil/2025/03/06/the-hidden-risks-of-open-source-ai-why-deepseek-r1s-transparency-isnt-enough/)  
67. Why DeepSeek's Open-Source AI is an Enterprise Security Risk | Cyber Magazine, accessed April 17, 2025, [https://cybermagazine.com/articles/why-deepseeks-open-source-ai-is-an-enterprise-security-risk](https://cybermagazine.com/articles/why-deepseeks-open-source-ai-is-an-enterprise-security-risk)  
68. Tamper-Resistant Safeguards for Open-Weight LLMs \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2408.00761v4](https://arxiv.org/html/2408.00761v4)  
69. Tamper-Resistant Safeguards for Open-Weight LLMs \- arXiv, accessed April 17, 2025, [https://arxiv.org/pdf/2408.00761](https://arxiv.org/pdf/2408.00761)  
70. Scaling Trends for Data Poisoning in LLMs \- AAAI Publications, accessed April 17, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/34929/37084](https://ojs.aaai.org/index.php/AAAI/article/view/34929/37084)  
71. On large language models safety, security, and privacy: A survey \- 中国光学期刊网, accessed April 17, 2025, [https://www.opticsjournal.net/Articles/OJba348e2553344135/FullText](https://www.opticsjournal.net/Articles/OJba348e2553344135/FullText)  
72. Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Trends \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2408.02946v5](https://arxiv.org/html/2408.02946v5)  
73. Why DeepSeek is a Chinese shot across Trump's bow \- Morningstar, accessed April 17, 2025, [https://www.morningstar.com/news/marketwatch/2025012898/why-deepseek-is-a-chinese-shot-across-trumps-bow](https://www.morningstar.com/news/marketwatch/2025012898/why-deepseek-is-a-chinese-shot-across-trumps-bow)  
74. DeepSeek and China's AI Innovation in US-China Tech Competition, accessed April 17, 2025, [https://www.cigionline.org/articles/deepseek-and-chinas-ai-innovation-in-us-china-tech-competition/](https://www.cigionline.org/articles/deepseek-and-chinas-ai-innovation-in-us-china-tech-competition/)  
75. Q\&A: DeepSeek AI assistant and the future of AI | Penn State University, accessed April 17, 2025, [https://www.psu.edu/news/research/story/qa-deepseek-ai-assistant-and-future-ai](https://www.psu.edu/news/research/story/qa-deepseek-ai-assistant-and-future-ai)  
76. DeepSeek-V3 Redefines LLM Performance and Cost Efficiency \- DeepLearning.AI, accessed April 17, 2025, [https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/](https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/)  
77. What DeepSeek Can Teach Us About AI Cost and Efficiency \- Unite.AI, accessed April 17, 2025, [https://www.unite.ai/what-deepseek-can-teach-us-about-ai-cost-and-efficiency/](https://www.unite.ai/what-deepseek-can-teach-us-about-ai-cost-and-efficiency/)  
78. \[D\] DeepSeek's $5.6M Training Cost: A Misleading Benchmark for AI Development? \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/MachineLearning/comments/1ibzsxa/d\_deepseeks\_56m\_training\_cost\_a\_misleading/](https://www.reddit.com/r/MachineLearning/comments/1ibzsxa/d_deepseeks_56m_training_cost_a_misleading/)  
79. AI Markets Were Deceived To Believe In DeepSeek's Low Training Costs; They Are Actually 400 Times Higher Than The Reported Figure, accessed April 17, 2025, [https://hardforum.com/threads/ai-markets-were-deceived-to-believe-in-deepseeks-low-training-costs-they-are-actually-400-times-higher-than-the-reported-figure.2039555/](https://hardforum.com/threads/ai-markets-were-deceived-to-believe-in-deepseeks-low-training-costs-they-are-actually-400-times-higher-than-the-reported-figure.2039555/)  
80. DeepSeek Debates: Chinese Leadership On Cost, True Training Cost, Closed Model Margin Impacts \- SemiAnalysis, accessed April 17, 2025, [https://semianalysis.com/2025/01/31/deepseek-debates/](https://semianalysis.com/2025/01/31/deepseek-debates/)  
81. Training AI for Pennies on the Dollar: Are DeepSeek's Costs Being Undersold? \- Sify, accessed April 17, 2025, [https://www.sify.com/ai-analytics/training-ai-for-pennies-on-the-dollar-are-deepseeks-costs-being-undersold/](https://www.sify.com/ai-analytics/training-ai-for-pennies-on-the-dollar-are-deepseeks-costs-being-undersold/)  
82. How to Install DeepSeek? What Models and Requirements Are Needed? \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLM/comments/1i6j3ih/how\_to\_install\_deepseek\_what\_models\_and/](https://www.reddit.com/r/LocalLLM/comments/1i6j3ih/how_to_install_deepseek_what_models_and/)  
83. DeepSeek-R1 671B: Complete Hardware Requirements \- DEV Community, accessed April 17, 2025, [https://dev.to/askyt/deepseek-r1-671b-complete-hardware-requirements-optimal-deployment-setup-2e48](https://dev.to/askyt/deepseek-r1-671b-complete-hardware-requirements-optimal-deployment-setup-2e48)  
84. Running DeepSeek LLM Models Locally on Your PC: Hardware Requirements and Deployment Guide \- Nova PC Builder, accessed April 17, 2025, [https://www.novapcbuilder.com/news/2025-02-05-running-deepseek-llm-models-locally-on-your-pc](https://www.novapcbuilder.com/news/2025-02-05-running-deepseek-llm-models-locally-on-your-pc)  
85. What hardware do I need to run DeepSeek locally? : r/LocalLLM \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLM/comments/1j9kguq/what\_hardware\_do\_i\_need\_to\_run\_deepseek\_locally/](https://www.reddit.com/r/LocalLLM/comments/1j9kguq/what_hardware_do_i_need_to_run_deepseek_locally/)  
86. deepseek-ai/DeepSeek-R1 · Hardware requirements? \- Hugging Face, accessed April 17, 2025, [https://huggingface.co/deepseek-ai/DeepSeek-R1/discussions/19](https://huggingface.co/deepseek-ai/DeepSeek-R1/discussions/19)  
87. DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements, accessed April 17, 2025, [https://dev.to/askyt/deepseek-r1-architecture-training-local-deployment-and-hardware-requirements-3mf8](https://dev.to/askyt/deepseek-r1-architecture-training-local-deployment-and-hardware-requirements-3mf8)  
88. Hardware requirements for running the large language model Deepseek R1 locally., accessed April 17, 2025, [https://www.rnfinity.com/news-show/Hardware-requirements-for-running-large-language-model-Deepseek-R1-on-a-local-machine](https://www.rnfinity.com/news-show/Hardware-requirements-for-running-large-language-model-Deepseek-R1-on-a-local-machine)  
89. DeepSeek R1 Hardware Requirements | Explained \- YouTube, accessed April 17, 2025, [https://www.youtube.com/watch?v=ASpGHOV6LEQ\&pp=0gcJCfcAhR29\_xXO](https://www.youtube.com/watch?v=ASpGHOV6LEQ&pp=0gcJCfcAhR29_xXO)  
90. Hardware required for Deepseek V3 671b? : r/LocalLLM \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLM/comments/1iz20k9/hardware\_required\_for\_deepseek\_v3\_671b/](https://www.reddit.com/r/LocalLLM/comments/1iz20k9/hardware_required_for_deepseek_v3_671b/)  
91. GPU Requirements Guide for DeepSeek Models (V3, All Variants) \- ApX Machine Learning, accessed April 17, 2025, [https://apxml.com/posts/system-requirements-deepseek-models](https://apxml.com/posts/system-requirements-deepseek-models)  
92. Deploying DeepSeek-R1 Locally: Complete Technical Guide (2025) \- Adyog, accessed April 17, 2025, [https://blog.adyog.com/2025/01/29/deploying-deepseek-r1-locally-complete-technical-guide-2025/](https://blog.adyog.com/2025/01/29/deploying-deepseek-r1-locally-complete-technical-guide-2025/)  
93. Run DeepSeek R1 locally on your device (Beginner-Friendly Guide) \- Jan.ai, accessed April 17, 2025, [https://jan.ai/post/deepseek-r1-locally](https://jan.ai/post/deepseek-r1-locally)  
94. Scalable and cost-effective fine-tuning for LLMs \- Red Hat, accessed April 17, 2025, [https://www.redhat.com/en/blog/how-to-achieve-scalable-cost-effective-fine-tuning-llm](https://www.redhat.com/en/blog/how-to-achieve-scalable-cost-effective-fine-tuning-llm)  
95. 5 Cheapest Cloud Platforms for Fine-tuning LLMs \- KDnuggets, accessed April 17, 2025, [https://www.kdnuggets.com/5-cheapest-cloud-platforms-for-fine-tuning-llms](https://www.kdnuggets.com/5-cheapest-cloud-platforms-for-fine-tuning-llms)  
96. Large Language Model Fine-Tuning via Context Stacking \- Winder.AI, accessed April 17, 2025, [https://winder.ai/large-language-model-fine-tuning-context-stacking/](https://winder.ai/large-language-model-fine-tuning-context-stacking/)  
97. China's DeepSeek labeled national security threat in bipartisan House committee report, accessed April 17, 2025, [https://siliconangle.com/2025/04/16/chinas-deepseek-labeled-national-security-threat-bipartisan-house-committee-report/](https://siliconangle.com/2025/04/16/chinas-deepseek-labeled-national-security-threat-bipartisan-house-committee-report/)  
98. U.S. House Panel Labels China's DeepSeek AI as National Security Threat, accessed April 17, 2025, [https://theoutpost.ai/news-story/us-considers-penalties-on-china-s-deep-seek-ai-tightens-restrictions-on-nvidia-chip-exports-14425/](https://theoutpost.ai/news-story/us-considers-penalties-on-china-s-deep-seek-ai-tightens-restrictions-on-nvidia-chip-exports-14425/)  
99. The report \- Select Committee on the CCP |, accessed April 17, 2025, [https://selectcommitteeontheccp.house.gov/sites/evo-subsites/selectcommitteeontheccp.house.gov/files/evo-media-document/DeepSeek%20Final.pdf](https://selectcommitteeontheccp.house.gov/sites/evo-subsites/selectcommitteeontheccp.house.gov/files/evo-media-document/DeepSeek%20Final.pdf)  
100. US gov't may block DeepSeek from US tech access, accessed April 17, 2025, [https://www.techinasia.com/news/bar-deepseek-chinese-ai-firm-tech-services](https://www.techinasia.com/news/bar-deepseek-chinese-ai-firm-tech-services)  
101. House Select Committee Says DeepSeek Is Threat to US Security | PYMNTS.com, accessed April 17, 2025, [https://www.pymnts.com/artificial-intelligence-2/2025/house-select-committee-says-deepseek-is-threat-to-us-security/](https://www.pymnts.com/artificial-intelligence-2/2025/house-select-committee-says-deepseek-is-threat-to-us-security/)  
102. Moolenaar, Krishnamoorthi Unveil Explosive Report on Chinese AI Firm DeepSeek — Demand Answers from Nvidia Over Chip Use | Select Committee on the CCP, accessed April 17, 2025, [https://selectcommitteeontheccp.house.gov/media/press-releases/moolenaar-krishnamoorthi-unveil-explosive-report-chinese-ai-firm-deepseek](https://selectcommitteeontheccp.house.gov/media/press-releases/moolenaar-krishnamoorthi-unveil-explosive-report-chinese-ai-firm-deepseek)  
103. Deepseek Unmasked: Exposing the CCP's Latest Tool For Spying, Stealing, and Subverting U.S. Export Control Restrictions | Select Committee on the CCP, accessed April 17, 2025, [https://selectcommitteeontheccp.house.gov/media/reports/deepseek-unmasked-exposing-ccps-latest-tool-spying-stealing-and-subverting-us-export](https://selectcommitteeontheccp.house.gov/media/reports/deepseek-unmasked-exposing-ccps-latest-tool-spying-stealing-and-subverting-us-export)  
104. House Panel Flags DeepSeek as Threat, Eyes Nvidia Sales \- PYMNTS.com, accessed April 17, 2025, [https://www.pymnts.com/cpi-posts/house-panel-flags-deepseek-as-threat-eyes-nvidia-sales/](https://www.pymnts.com/cpi-posts/house-panel-flags-deepseek-as-threat-eyes-nvidia-sales/)  
105. U.S. targets Chinese AI firm DeepSeek over security concerns \- aju press, accessed April 17, 2025, [https://www.ajupress.com/view/20250417153206621](https://www.ajupress.com/view/20250417153206621)  
106. 85% of DeepSeek responses manipulated to enforce CCP narratives: US House report, accessed April 17, 2025, [https://www.taipeitimes.com/News/taiwan/archives/2025/04/17/2003835353](https://www.taipeitimes.com/News/taiwan/archives/2025/04/17/2003835353)  
107. AI startup DeepSeek facing hack, blocks questions about CCP \- Fox Business, accessed April 17, 2025, [https://www.foxbusiness.com/technology/ai-startup-deepseek-facing-hack-blocks-questions-about-ccp](https://www.foxbusiness.com/technology/ai-startup-deepseek-facing-hack-blocks-questions-about-ccp)  
108. China \- Global Policy Watch, accessed April 17, 2025, [https://www.globalpolicywatch.com/category/china/](https://www.globalpolicywatch.com/category/china/)  
109. March 2025 AI Developments Under the Trump Administration, accessed April 17, 2025, [https://www.insidegovernmentcontracts.com/2025/04/march-2025-ai-developments-under-the-trump-administration/](https://www.insidegovernmentcontracts.com/2025/04/march-2025-ai-developments-under-the-trump-administration/)  
110. DeepSeek: Latest News and Updates | South China Morning Post, accessed April 17, 2025, [https://www.scmp.com/topics/deepseek](https://www.scmp.com/topics/deepseek)  
111. Hawkish AI? Uncovering DeepSeek's Foreign Policy Biases \- CSIS, accessed April 17, 2025, [https://www.csis.org/analysis/hawkish-ai-uncovering-deepseeks-foreign-policy-biases](https://www.csis.org/analysis/hawkish-ai-uncovering-deepseeks-foreign-policy-biases)  
112. 3028335560-files.gitbook.io, accessed April 17, 2025, [https://3028335560-files.gitbook.io/\~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FlJdZs7NyMJ6Ewm4U1eRP%2Fuploads%2FOVdpd7QoNIDAZdfpGrGV%2FAethir%20Whitepaper.pdf?alt=media\&token=ec38bfde-1668-472d-97d7-a48fb1300703](https://3028335560-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FlJdZs7NyMJ6Ewm4U1eRP%2Fuploads%2FOVdpd7QoNIDAZdfpGrGV%2FAethir%20Whitepaper.pdf?alt=media&token=ec38bfde-1668-472d-97d7-a48fb1300703)  
113. Understanding U.S. Allies' Current Legal Authority to Implement AI and Semiconductor Export Controls \- CSIS, accessed April 17, 2025, [https://www.csis.org/analysis/understanding-us-allies-current-legal-authority-implement-ai-and-semiconductor-export](https://www.csis.org/analysis/understanding-us-allies-current-legal-authority-implement-ai-and-semiconductor-export)  
114. DeepSeek AI, accessed April 17, 2025, [https://deepseek.ai/](https://deepseek.ai/)  
115. DeepSeek Shakes AI Industry as China's Open-Source Model Gains Ground, accessed April 17, 2025, [https://www.fintechweekly.com/magazine/articles/deepseek-shakes-ai-industry](https://www.fintechweekly.com/magazine/articles/deepseek-shakes-ai-industry)  
116. DeepSeek R1: All you need to know \- Fireworks AI, accessed April 17, 2025, [https://fireworks.ai/blog/deepseek-r1-deepdive](https://fireworks.ai/blog/deepseek-r1-deepdive)  
117. deepseek-ai/DeepSeek-R1 · Hugging Face, accessed April 17, 2025, [https://huggingface.co/deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)  
118. DeepSeek, Huawei, Export Controls, and the Future of the U.S.-China AI Race \- CSIS, accessed April 17, 2025, [https://www.csis.org/analysis/deepseek-huawei-export-controls-and-future-us-china-ai-race](https://www.csis.org/analysis/deepseek-huawei-export-controls-and-future-us-china-ai-race)  
119. DeepSeek AI Is the Competition America Needs \- Technology & Democracy Project, accessed April 17, 2025, [https://www.discovery.org/tech/2025/01/31/deepseek-ai-is-the-competition-america-needs/](https://www.discovery.org/tech/2025/01/31/deepseek-ai-is-the-competition-america-needs/)  
120. \[2401.02954\] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism, accessed April 17, 2025, [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954)  
121. DeepSeek LLM Scaling Open-Source Language Models with Longtermism \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2401.02954v1](https://arxiv.org/html/2401.02954v1)  
122. DeepSeek AI: Advancing Open-Source LLMs with MoE & Reinforcement Learning \- Inferless, accessed April 17, 2025, [https://www.inferless.com/learn/the-ultimate-guide-to-deepseek-models](https://www.inferless.com/learn/the-ultimate-guide-to-deepseek-models)  
123. Big misconceptions of training costs for Deepseek and OpenAI : r/singularity \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/singularity/comments/1id60qi/big\_misconceptions\_of\_training\_costs\_for\_deepseek/](https://www.reddit.com/r/singularity/comments/1id60qi/big_misconceptions_of_training_costs_for_deepseek/)  
124. What is DeepSeek AI? (Features, OpenAI Comparison, & More) \- Exploding Topics, accessed April 17, 2025, [https://explodingtopics.com/blog/deepseek-ai](https://explodingtopics.com/blog/deepseek-ai)  
125. DeepSeek AI Chat, accessed April 17, 2025, [https://deep-seek.chat/](https://deep-seek.chat/)  
126. DeepSeek V2 vs Coder V2: A Comparative Analysis' \- PromptLayer, accessed April 17, 2025, [https://blog.promptlayer.com/deepseek-v2-vs-coder-v2-a-comparative-analysis/](https://blog.promptlayer.com/deepseek-v2-vs-coder-v2-a-comparative-analysis/)  
127. deepseek-ai/DeepSeek-Coder-V2-Instruct · Hugging Face, accessed April 17, 2025, [https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)  
128. ‍ LLM Comparison/Test: DeepSeek-V3, QVQ-72B-Preview, Falcon3 10B, Llama 3.3 70B, Nemotron 70B in my updated MMLU-Pro CS benchmark \- Hugging Face, accessed April 17, 2025, [https://huggingface.co/blog/wolfram/llm-comparison-test-2025-01-02](https://huggingface.co/blog/wolfram/llm-comparison-test-2025-01-02)  
129. DeepSeek R1 Research Paper Audio Summary : r/singularity \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/singularity/comments/1iadqq3/deepseek\_r1\_research\_paper\_audio\_summary/](https://www.reddit.com/r/singularity/comments/1iadqq3/deepseek_r1_research_paper_audio_summary/)  
130. \[2502.02523\] Brief analysis of DeepSeek R1 and its implications for Generative AI \- arXiv, accessed April 17, 2025, [https://arxiv.org/abs/2502.02523](https://arxiv.org/abs/2502.02523)  
131. DeepSeek-R1: Features, o1 Comparison, Distilled Models & More | DataCamp, accessed April 17, 2025, [https://www.datacamp.com/blog/deepseek-r1](https://www.datacamp.com/blog/deepseek-r1)  
132. \[2502.14837\] Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs \- arXiv, accessed April 17, 2025, [https://arxiv.org/abs/2502.14837](https://arxiv.org/abs/2502.14837)  
133. \[2502.07864\] TransMLA: Multi-Head Latent Attention Is All You Need \- arXiv, accessed April 17, 2025, [https://arxiv.org/abs/2502.07864](https://arxiv.org/abs/2502.07864)  
134. TransMLA: Multi-head Latent Attention Is All You Need \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2502.07864v1](https://arxiv.org/html/2502.07864v1)  
135. Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2503.15092v1](https://arxiv.org/html/2503.15092v1)  
136. The State of DePIN | PDF | Computer Network | Internet Of Things \- Scribd, accessed April 17, 2025, [https://www.scribd.com/document/793462752/The-State-of-DePIN](https://www.scribd.com/document/793462752/The-State-of-DePIN)  
137. Got DeepSeek R1 running locally \- Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??) : r/selfhosted \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/selfhosted/comments/1i6ggyh/got\_deepseek\_r1\_running\_locally\_full\_setup\_guide/](https://www.reddit.com/r/selfhosted/comments/1i6ggyh/got_deepseek_r1_running_locally_full_setup_guide/)  
138. How to install Deepseek on Ollama \- Hostinger Help Center, accessed April 17, 2025, [https://support.hostinger.com/en/articles/10506050-how-to-install-deepseek-on-ollama](https://support.hostinger.com/en/articles/10506050-how-to-install-deepseek-on-ollama)  
139. Setting Up Ollama & Running DeepSeek R1 Locally for a Powerful RAG System, accessed April 17, 2025, [https://dev.to/ajmal\_hasan/setting-up-ollama-running-deepseek-r1-locally-for-a-powerful-rag-system-4pd4](https://dev.to/ajmal_hasan/setting-up-ollama-running-deepseek-r1-locally-for-a-powerful-rag-system-4pd4)  
140. deepseek · Ollama Search, accessed April 17, 2025, [https://ollama.com/search?q=deepseek](https://ollama.com/search?q=deepseek)  
141. How to Install and Run DeepSeek R1 Locally With vLLM V1 \- Database Mart, accessed April 17, 2025, [https://www.databasemart.com/blog/install-and-run-deepseek-r1-locally-with-vllm-v1](https://www.databasemart.com/blog/install-and-run-deepseek-r1-locally-with-vllm-v1)  
142. Local Deployment Guide for DeepSeek V3: From Basics to Advanced \- Chat Stream, accessed April 17, 2025, [https://www.chatstream.org/en/blog/deepseek-deploy-guide](https://www.chatstream.org/en/blog/deepseek-deploy-guide)  
143. DeepSeek-V3 \+ SGLang: Inference Optimization — Blog \- DataCrunch, accessed April 17, 2025, [https://datacrunch.io/blog/deepseek-v3-sglang-inference-optimization](https://datacrunch.io/blog/deepseek-v3-sglang-inference-optimization)  
144. How to Set Up and Optimize DeepSeek Locally | Built In, accessed April 17, 2025, [https://builtin.com/artificial-intelligence/how-implement-deepseek-locally](https://builtin.com/artificial-intelligence/how-implement-deepseek-locally)  
145. DeepSeek Privacy Policy, accessed April 17, 2025, [https://chat.deepseek.com/downloads/DeepSeek%20Privacy%20Policy.pdf](https://chat.deepseek.com/downloads/DeepSeek%20Privacy%20Policy.pdf)  
146. DeepSeek App: A Closer Look at Its Privacy Posture \- Privado.ai, accessed April 17, 2025, [https://www.privado.ai/post/deepseek-app-a-closer-look-at-its-privacy-posture](https://www.privado.ai/post/deepseek-app-a-closer-look-at-its-privacy-posture)  
147. DeepSeek AI. What IT Security Leaders Need to Know \- Ironscales, accessed April 17, 2025, [https://ironscales.com/blog/deepseek-ai.-what-it-security-leaders-need-to-know](https://ironscales.com/blog/deepseek-ai.-what-it-security-leaders-need-to-know)  
148. About DeepSeek V3 privacy concern : r/LocalLLaMA \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1hvp5z1/about\_deepseek\_v3\_privacy\_concern/](https://www.reddit.com/r/LocalLLaMA/comments/1hvp5z1/about_deepseek_v3_privacy_concern/)  
149. DeepSeek privacy terms : r/WritingWithAI \- Reddit, accessed April 17, 2025, [https://www.reddit.com/r/WritingWithAI/comments/1icswgv/deepseek\_privacy\_terms/](https://www.reddit.com/r/WritingWithAI/comments/1icswgv/deepseek_privacy_terms/)  
150. DeepSeek Coder Privacy Policy, accessed April 17, 2025, [https://chat.deepseek.com/downloads/DeepSeek%20Coder%20Privacy%20Policy\_1019.pdf](https://chat.deepseek.com/downloads/DeepSeek%20Coder%20Privacy%20Policy_1019.pdf)  
151. DeepSeek User Agreement, accessed April 17, 2025, [https://chat.deepseek.com/downloads/DeepSeek%20User%20Agreement.pdf](https://chat.deepseek.com/downloads/DeepSeek%20User%20Agreement.pdf)  
152. DeepSeek Terms of Use, accessed April 17, 2025, [https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html](https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html)  
153. DeepSeek Explained: What Is It and Is It Safe To Use? | News \- AI@ND, accessed April 17, 2025, [https://ai.nd.edu/news/deepseek-explained-what-is-it-and-is-it-safe-to-use/](https://ai.nd.edu/news/deepseek-explained-what-is-it-and-is-it-safe-to-use/)  
154. www.nowsecure.com, accessed April 17, 2025, [https://www.nowsecure.com/blog/2025/02/06/nowsecure-uncovers-multiple-security-and-privacy-flaws-in-deepseek-ios-mobile-app/\#:\~:text=The%20DeepSeek%20iOS%20app%20sends,users%20of%20the%20DeepSeek%20app.](https://www.nowsecure.com/blog/2025/02/06/nowsecure-uncovers-multiple-security-and-privacy-flaws-in-deepseek-ios-mobile-app/#:~:text=The%20DeepSeek%20iOS%20app%20sends,users%20of%20the%20DeepSeek%20app.)  
155. The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1 \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2502.12659v1](https://arxiv.org/html/2502.12659v1)  
156. Safety Evaluation of DeepSeek Models in Chinese Contexts \- arXiv, accessed April 17, 2025, [https://arxiv.org/html/2502.11137v2](https://arxiv.org/html/2502.11137v2)  
157. GPT-4o Guardrails Gone: Data Poisoning & Jailbreak-Tuning | FAR.AI, accessed April 17, 2025, [https://far.ai/post/2024-10-poisoning/](https://far.ai/post/2024-10-poisoning/)  
158. \[BUG\] Insecure Data Processing \- Timing Attack Against Secret \- High (7.4) \#649 \- GitHub, accessed April 17, 2025, [https://github.com/deepseek-ai/DeepSeek-V3/issues/649](https://github.com/deepseek-ai/DeepSeek-V3/issues/649)  
159. DeepSeek API: A Guide With Examples and Cost Calculations \- DataCamp, accessed April 17, 2025, [https://www.datacamp.com/tutorial/deepseek-api](https://www.datacamp.com/tutorial/deepseek-api)  
160. Models & Pricing \- DeepSeek API Docs, accessed April 17, 2025, [https://api-docs.deepseek.com/quick\_start/pricing](https://api-docs.deepseek.com/quick_start/pricing)  
161. DeepSeek Pricing: An Affordable AI Solution \- Lark, accessed April 17, 2025, [https://www.larksuite.com/en\_us/blog/deepseek-pricing](https://www.larksuite.com/en_us/blog/deepseek-pricing)  
162. DeepSeek Ends Promotional API Pricing Amidst Demand Surge \- Silicon UK, accessed April 17, 2025, [https://www.silicon.co.uk/cloud/ai/deepseek-api-pricing-599077](https://www.silicon.co.uk/cloud/ai/deepseek-api-pricing-599077)  
163. US plans to block DeepSeek from buying technology: NYT \- Newspaper \- DAWN.COM, accessed April 17, 2025, [https://www.dawn.com/news/1904726/us-plans-to-block-deepseek-from-buying-technology-nyt](https://www.dawn.com/news/1904726/us-plans-to-block-deepseek-from-buying-technology-nyt)