from models.llm import llm  # LLM í”„ë¡ì‹œ í•¨ìˆ˜

def generate_direct_answer(state: dict) -> dict:
    # """ì‚¬ìš©ì ì§ˆë¬¸ì— ë„êµ¬ ì—†ì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤."""
    # print(f"[DEBUG] state íƒ€ì…: {type(state)}, ê°’: {state}")

    # system_prompt = "ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë„êµ¬ ì—†ì´ ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”."
    # user_input = state.get("input", "")

    # result = llm.invoke(user_input, system_prompt)
    # answer = result.get("content") if isinstance(result, dict) else result.content

    return {
        **state,
        "output": f"ğŸ§  ì§ì ‘ ì‘ë‹µ:\n\n{answer}",
        "status": "ğŸ“Œ ì§ì ‘ ì‘ë‹µ ì™„ë£Œ"
    }
