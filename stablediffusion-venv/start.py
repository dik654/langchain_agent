from fastapi import FastAPI
from pydantic import BaseModel
from diffusers import StableDiffusionXLPipeline
import torch
from PIL import Image
import io
import base64
import os

# ì˜¤í”„ë¼ì¸ ì‹¤í–‰ ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = "2,3"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

# FastAPI ì¸ìŠ¤í„´ìŠ¤
app = FastAPI()

# ìš”ì²­ ëª¨ë¸
class PromptRequest(BaseModel):
    prompt: str

# ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "/home/filadmin/ai/models/stable-diffusion-3.5-large"

# âœ… ëª¨ë¸ ë¡œë”©
print("ğŸš€ Stable Diffusion ë¡œë”© ì¤‘...")
pipe = StableDiffusionXLPipeline.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    use_safetensors=True,
    local_files_only=True
)

# âœ… U-Netì„ 2GPUì— ë¶„ì‚°
if torch.cuda.device_count() < 2:
    raise RuntimeError("âš ï¸ ìµœì†Œ 2ê°œì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.")

pipe.unet = torch.nn.DataParallel(pipe.unet, device_ids=[0, 1])
pipe.to("cuda")

# âœ… Warmup (ìµœì´ˆ ì¶”ë¡  ì‹œê°„ ë‹¨ì¶•)
_ = pipe("warmup test").images[0]
print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

# âœ… POST /generate
@app.post("/generate")
def generate(req: PromptRequest):
    try:
        image = pipe(req.prompt).images[0]
        buffer = io.BytesIO()
        image.save(buffer, format="PNG")
        img_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

        return {
            "status": "ok",
            "prompt": req.prompt,
            "image_base64": img_base64
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}

